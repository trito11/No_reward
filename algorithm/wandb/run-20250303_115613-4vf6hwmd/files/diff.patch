diff --git a/algorithm/buffers.py b/algorithm/buffers.py
index 4f8b037..5566e7a 100644
--- a/algorithm/buffers.py
+++ b/algorithm/buffers.py
@@ -470,7 +470,7 @@ class TrajectoryReplayBuffer(BaseBuffer):
             self.timeouts[self.eps][self.pos] = np.array([info.get("TimeLimit.truncated", False) for info in infos])
 
         self.pos += 1
-        if  "final_info" in infos:
+        if  "final_info" in infos or self.pos==100:
             self.lenght[self.eps] = self.pos
             # print(self.pos)
             # print(infos["final_info"][0]["episode"]["l"])
@@ -505,13 +505,15 @@ class TrajectoryReplayBuffer(BaseBuffer):
         # Do not sample the element with index `self.pos` as the transitions is invalid
         # (we use only one array to store `obs` and `next_obs`)
         if self.full:
+
             eps_id = np.random.randint(self.trajectory_size)
             eps_len = self.lenght[eps_id]
-            batch_inds = np.random.randint(0, eps_len, size=batch_size)
+            batch_inds =  np.random.choice(eps_len, size=batch_size, replace=(batch_size > eps_len))
+
         else:
             eps_id = np.random.randint(self.eps)
             eps_len = self.lenght[eps_id]
-            batch_inds = np.random.randint(0, eps_len, size=batch_size)
+            batch_inds = np.random.choice(eps_len, size=batch_size, replace=(batch_size > eps_len))
             # if batch_size>=eps_len:
             #     rrd_var_coef_ph = 1
             # else: rrd_var_coef_ph = 1.0-float(batch_size)/eps_len
diff --git a/algorithm/sac_rrd_iq.py b/algorithm/sac_rrd_iq.py
index e25ca4e..2886269 100644
--- a/algorithm/sac_rrd_iq.py
+++ b/algorithm/sac_rrd_iq.py
@@ -26,7 +26,7 @@ class Args:
     """if toggled, `torch.backends.cudnn.deterministic=False`"""
     cuda: bool = True
     """if toggled, cuda will be enabled by default"""
-    track: bool = False
+    track: bool = True
     """if toggled, this experiment will be tracked with Weights and Biases"""
     wandb_project_name: str = "cleanRL"
     """the wandb's project name"""
diff --git a/algorithm/wandb/latest-run b/algorithm/wandb/latest-run
index 319a787..51f48b2 120000
--- a/algorithm/wandb/latest-run
+++ b/algorithm/wandb/latest-run
@@ -1 +1 @@
-run-20250223_194552-qtgzrth9
\ No newline at end of file
+run-20250303_115613-4vf6hwmd
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index ddc0212..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,462 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + 2 * r_var1 + 0.5 * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + 2 * r_var2 + 0.5 * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/conda-environment.yaml b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/config.yaml b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/config.yaml
deleted file mode 100644
index 52ccec1..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/config.yaml
+++ /dev/null
@@ -1,94 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1738566988.292697
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_lr:
-  desc: null
-  value: 0.001
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/diff.patch b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/events.out.tfevents.1738566994.Tri.714867.0 b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/events.out.tfevents.1738566994.Tri.714867.0
deleted file mode 120000
index 65a5137..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/events.out.tfevents.1738566994.Tri.714867.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd_iq__1__1738566987/events.out.tfevents.1738566994.Tri.714867.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/requirements.txt b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/wandb-metadata.json b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/wandb-metadata.json
deleted file mode 100644
index 56b6fd3..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-03T07:16:31.893009",
-    "startedAt": "2025-02-03T07:16:28.284444",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 16.713085174560547
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/wandb-summary.json b/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/wandb-summary.json
deleted file mode 100644
index 3e4b524..0000000
--- a/algorithm/wandb/run-20250203_141628-9wmxg5nf/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 59801, "_timestamp": 1738567677.8577642, "_runtime": 689.5650672912598, "_step": 1126, "charts/episodic_return": 366.4917907714844, "charts/episodic_length": 205.0, "losses/qf1_values": 131.7149200439453, "losses/qf2_values": 133.994140625, "losses/qf1_loss": 218.99826049804688, "losses/qf2_loss": 139.5494384765625, "losses/qf_loss": 179.2738494873047, "losses/actor_loss": -141.23207092285156, "losses/alpha": 0.07560358941555023, "losses/re_loss": 0.00011564828309928998, "rewards/q_r_loss": 61.54500961303711, "rewards/pre_re": -1.6694762706756592, "rewards/pre_rewards": 0.8052802085876465, "rewards/eps_rewards": 0.8142123222351074, "rewards/batch_rewards": 0.8114267587661743, "rewards/true_r_var": 0.006588442716747522, "rewards/a_var1": 85.99459838867188, "rewards/a_var2": 55.31621170043945, "rewards/r_var1": 86.453369140625, "rewards/r_var2": 55.27786636352539, "rewards/r_q_v_max": 20.980478286743164, "rewards/r_q_v_min": -18.02611541748047, "rewards/r_true_max": 0.9617781639099121, "rewards/r_true_min": 0.6720705628395081, "charts/SPS": 87.0, "losses/alpha_loss": -0.038730137050151825, "_wandb": {"runtime": 688}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_141628-9wmxg5nf/run-9wmxg5nf.wandb b/algorithm/wandb/run-20250203_141628-9wmxg5nf/run-9wmxg5nf.wandb
deleted file mode 100644
index f58a1c6..0000000
Binary files a/algorithm/wandb/run-20250203_141628-9wmxg5nf/run-9wmxg5nf.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250203_142822-dct31l3f/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index ddc0212..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,462 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + 2 * r_var1 + 0.5 * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + 2 * r_var2 + 0.5 * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/conda-environment.yaml b/algorithm/wandb/run-20250203_142822-dct31l3f/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/config.yaml b/algorithm/wandb/run-20250203_142822-dct31l3f/files/config.yaml
deleted file mode 100644
index 7e2f844..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/config.yaml
+++ /dev/null
@@ -1,94 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1738567702.273821
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_lr:
-  desc: null
-  value: 0.001
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/diff.patch b/algorithm/wandb/run-20250203_142822-dct31l3f/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/events.out.tfevents.1738567715.Tri.720128.0 b/algorithm/wandb/run-20250203_142822-dct31l3f/files/events.out.tfevents.1738567715.Tri.720128.0
deleted file mode 120000
index 76e41a0..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/events.out.tfevents.1738567715.Tri.720128.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd_iq__1__1738567697/events.out.tfevents.1738567715.Tri.720128.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/requirements.txt b/algorithm/wandb/run-20250203_142822-dct31l3f/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/wandb-metadata.json b/algorithm/wandb/run-20250203_142822-dct31l3f/files/wandb-metadata.json
deleted file mode 100644
index 25ee30b..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-03T07:28:32.411213",
-    "startedAt": "2025-02-03T07:28:22.269335",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 16.715694427490234
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/files/wandb-summary.json b/algorithm/wandb/run-20250203_142822-dct31l3f/files/wandb-summary.json
deleted file mode 100644
index 614f6cd..0000000
--- a/algorithm/wandb/run-20250203_142822-dct31l3f/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2999900, "_timestamp": 1738634501.6938722, "_runtime": 66799.42005109787, "_step": 34278, "charts/episodic_return": 3179.349609375, "charts/episodic_length": 1000.0, "losses/qf1_values": 197.67710876464844, "losses/qf2_values": 196.66754150390625, "losses/qf1_loss": 5.950131416320801, "losses/qf2_loss": 5.295663833618164, "losses/qf_loss": 5.622897624969482, "losses/actor_loss": -196.81222534179688, "losses/alpha": 0.024675950407981873, "losses/re_loss": 0.001298199174925685, "rewards/q_r_loss": 2.424232006072998, "rewards/pre_re": 1.884366512298584, "rewards/pre_rewards": 3.2027721405029297, "rewards/eps_rewards": 3.2379214763641357, "rewards/batch_rewards": 3.2278711795806885, "rewards/true_r_var": 0.35971495509147644, "rewards/a_var1": 1.1602920293807983, "rewards/a_var2": 1.2353334426879883, "rewards/r_var1": 0.594898521900177, "rewards/r_var2": 0.6955441236495972, "rewards/r_q_v_max": 4.4275360107421875, "rewards/r_q_v_min": -1.0053253173828125, "rewards/r_true_max": 4.568378448486328, "rewards/r_true_min": 1.0786203145980835, "charts/SPS": 44.0, "losses/alpha_loss": 0.022853340953588486, "_wandb": {"runtime": 66794}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_142822-dct31l3f/run-dct31l3f.wandb b/algorithm/wandb/run-20250203_142822-dct31l3f/run-dct31l3f.wandb
deleted file mode 100644
index c1eb165..0000000
Binary files a/algorithm/wandb/run-20250203_142822-dct31l3f/run-dct31l3f.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 6b77a51..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,462 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + 1 * r_var1 + 0 * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + 1 * r_var2 + 0 * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/conda-environment.yaml b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/config.yaml b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/config.yaml
deleted file mode 100644
index 39e6a5f..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/config.yaml
+++ /dev/null
@@ -1,94 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1738582027.74321
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_lr:
-  desc: null
-  value: 0.001
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/diff.patch b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/events.out.tfevents.1738582032.Tri.791568.0 b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/events.out.tfevents.1738582032.Tri.791568.0
deleted file mode 120000
index bf6d172..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/events.out.tfevents.1738582032.Tri.791568.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd_iq__1__1738582023/events.out.tfevents.1738582032.Tri.791568.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/requirements.txt b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/wandb-metadata.json b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/wandb-metadata.json
deleted file mode 100644
index 9569560..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-03T11:27:08.990324",
-    "startedAt": "2025-02-03T11:27:07.732525",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 16.753963470458984
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/wandb-summary.json b/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/wandb-summary.json
deleted file mode 100644
index d435b65..0000000
--- a/algorithm/wandb/run-20250203_182707-qtr6kw3z/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2999900, "_timestamp": 1738660474.6795537, "_runtime": 78446.93634366989, "_step": 34188, "charts/episodic_return": 3403.2666015625, "charts/episodic_length": 1000.0, "losses/qf1_values": 295.16943359375, "losses/qf2_values": 292.82855224609375, "losses/qf1_loss": 92.54934692382812, "losses/qf2_loss": 85.90971374511719, "losses/qf_loss": 89.22953033447266, "losses/actor_loss": -299.3169250488281, "losses/alpha": 0.026815034449100494, "losses/re_loss": 0.0005610290099866688, "rewards/q_r_loss": 90.19729614257812, "rewards/pre_re": -1.4178950786590576, "rewards/pre_rewards": 1.4100111722946167, "rewards/eps_rewards": 1.3882358074188232, "rewards/batch_rewards": 1.3700120449066162, "rewards/true_r_var": 0.06577924638986588, "rewards/a_var1": 93.0549087524414, "rewards/a_var2": 83.85826110839844, "rewards/r_var1": 92.12316131591797, "rewards/r_var2": 83.06017303466797, "rewards/r_q_v_max": 11.649322509765625, "rewards/r_q_v_min": -46.16230773925781, "rewards/r_true_max": 1.8664124011993408, "rewards/r_true_min": 0.8757184147834778, "charts/SPS": 38.0, "losses/alpha_loss": -0.1278388500213623, "_wandb": {"runtime": 78447}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182707-qtr6kw3z/run-qtr6kw3z.wandb b/algorithm/wandb/run-20250203_182707-qtr6kw3z/run-qtr6kw3z.wandb
deleted file mode 100644
index a0525d1..0000000
Binary files a/algorithm/wandb/run-20250203_182707-qtr6kw3z/run-qtr6kw3z.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 6b77a51..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,462 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + 1 * r_var1 + 0 * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + 1 * r_var2 + 0 * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/conda-environment.yaml b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/config.yaml b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/config.yaml
deleted file mode 100644
index 068cfbd..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/config.yaml
+++ /dev/null
@@ -1,94 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1738582051.504065
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_lr:
-  desc: null
-  value: 0.001
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/diff.patch b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/events.out.tfevents.1738582056.Tri.791972.0 b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/events.out.tfevents.1738582056.Tri.791972.0
deleted file mode 120000
index 023084b..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/events.out.tfevents.1738582056.Tri.791972.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd_iq__1__1738582047/events.out.tfevents.1738582056.Tri.791972.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/requirements.txt b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/wandb-metadata.json b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/wandb-metadata.json
deleted file mode 100644
index 19f6a45..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-03T11:27:32.854105",
-    "startedAt": "2025-02-03T11:27:31.488199",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 16.754417419433594
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/wandb-summary.json b/algorithm/wandb/run-20250203_182731-t8mviyl1/files/wandb-summary.json
deleted file mode 100644
index a51b7fc..0000000
--- a/algorithm/wandb/run-20250203_182731-t8mviyl1/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2999900, "_timestamp": 1738660439.8113933, "_runtime": 78388.30732822418, "_step": 34189, "charts/episodic_return": 3403.2666015625, "charts/episodic_length": 1000.0, "losses/qf1_values": 295.16943359375, "losses/qf2_values": 292.82855224609375, "losses/qf1_loss": 92.54934692382812, "losses/qf2_loss": 85.90971374511719, "losses/qf_loss": 89.22953033447266, "losses/actor_loss": -299.3169250488281, "losses/alpha": 0.026815034449100494, "losses/re_loss": 0.0005610290099866688, "rewards/q_r_loss": 90.19729614257812, "rewards/pre_re": -1.4178950786590576, "rewards/pre_rewards": 1.4100111722946167, "rewards/eps_rewards": 1.3882358074188232, "rewards/batch_rewards": 1.3700120449066162, "rewards/true_r_var": 0.06577924638986588, "rewards/a_var1": 93.0549087524414, "rewards/a_var2": 83.85826110839844, "rewards/r_var1": 92.12316131591797, "rewards/r_var2": 83.06017303466797, "rewards/r_q_v_max": 11.649322509765625, "rewards/r_q_v_min": -46.16230773925781, "rewards/r_true_max": 1.8664124011993408, "rewards/r_true_min": 0.8757184147834778, "charts/SPS": 38.0, "losses/alpha_loss": -0.1278388500213623, "_wandb": {"runtime": 78389}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250203_182731-t8mviyl1/run-t8mviyl1.wandb b/algorithm/wandb/run-20250203_182731-t8mviyl1/run-t8mviyl1.wandb
deleted file mode 100644
index 131685e..0000000
Binary files a/algorithm/wandb/run-20250203_182731-t8mviyl1/run-t8mviyl1.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 56dd5ab..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/conda-environment.yaml b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/config.yaml b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/config.yaml
deleted file mode 100644
index 67839a6..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1738609388.458272
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/diff.patch b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/events.out.tfevents.1738609395.Tri.956634.0 b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/events.out.tfevents.1738609395.Tri.956634.0
deleted file mode 120000
index 6c5de77..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/events.out.tfevents.1738609395.Tri.956634.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd_iq__1__1738609384/events.out.tfevents.1738609395.Tri.956634.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/requirements.txt b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/wandb-metadata.json b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/wandb-metadata.json
deleted file mode 100644
index 312a37b..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-03T19:03:10.948347",
-    "startedAt": "2025-02-03T19:03:08.448373",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 16.87877655029297
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/wandb-summary.json b/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/wandb-summary.json
deleted file mode 100644
index ce4e070..0000000
--- a/algorithm/wandb/run-20250204_020308-0zt6fe2z/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2999900, "_timestamp": 1738673314.2019281, "_runtime": 63925.74365615845, "_step": 34172, "charts/episodic_return": 3403.2666015625, "charts/episodic_length": 1000.0, "losses/qf1_values": 295.16943359375, "losses/qf2_values": 292.82855224609375, "losses/qf1_loss": 92.54934692382812, "losses/qf2_loss": 85.90971374511719, "losses/qf_loss": 89.22953033447266, "losses/actor_loss": -299.3169250488281, "losses/alpha": 0.026815034449100494, "losses/re_loss": 0.0005610290099866688, "rewards/q_r_loss": 90.19729614257812, "rewards/pre_re": -1.4178950786590576, "rewards/pre_rewards": 1.4100111722946167, "rewards/eps_rewards": 1.3882358074188232, "rewards/batch_rewards": 1.3700120449066162, "rewards/true_r_var": 0.06577924638986588, "rewards/a_var1": 93.0549087524414, "rewards/a_var2": 83.85826110839844, "rewards/r_var1": 92.12316131591797, "rewards/r_var2": 83.06017303466797, "rewards/r_q_v_max": 11.649322509765625, "rewards/r_q_v_min": -46.16230773925781, "rewards/r_true_max": 1.8664124011993408, "rewards/r_true_min": 0.8757184147834778, "charts/SPS": 46.0, "losses/alpha_loss": -0.1278388500213623, "_wandb": {"runtime": 63924}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250204_020308-0zt6fe2z/run-0zt6fe2z.wandb b/algorithm/wandb/run-20250204_020308-0zt6fe2z/run-0zt6fe2z.wandb
deleted file mode 100644
index bad3c21..0000000
Binary files a/algorithm/wandb/run-20250204_020308-0zt6fe2z/run-0zt6fe2z.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index e1d633f..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/conda-environment.yaml b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/config.yaml b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/config.yaml
deleted file mode 100644
index e3104e2..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739365435.96074
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/diff.patch b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/events.out.tfevents.1739365443.Tri.1624.0 b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/events.out.tfevents.1739365443.Tri.1624.0
deleted file mode 120000
index 97f7a73..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/events.out.tfevents.1739365443.Tri.1624.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739365429/events.out.tfevents.1739365443.Tri.1624.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/requirements.txt b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/wandb-metadata.json b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/wandb-metadata.json
deleted file mode 100644
index 37cdfd2..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:03:59.407020",
-    "startedAt": "2025-02-12T13:03:55.954140",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.171466827392578
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/wandb-summary.json b/algorithm/wandb/run-20250212_200355-zwogpj4b/files/wandb-summary.json
deleted file mode 100644
index 6f00060..0000000
--- a/algorithm/wandb/run-20250212_200355-zwogpj4b/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 135600, "_timestamp": 1739367237.8943021, "_runtime": 1801.933562040329, "_step": 1525, "charts/episodic_return": -12.829331398010254, "charts/episodic_length": 66.0, "losses/qf1_values": 28.660629272460938, "losses/qf2_values": 29.4576416015625, "losses/qf1_loss": 2.1889030933380127, "losses/qf2_loss": 2.4449288845062256, "losses/qf_loss": 2.316915988922119, "losses/actor_loss": -29.453767776489258, "losses/alpha": 0.015297839418053627, "losses/re_loss": 6.598668846891087e-07, "rewards/q_r_loss": 2.3831353187561035, "rewards/pre_re": -0.7443057298660278, "rewards/pre_rewards": -0.3754952549934387, "rewards/eps_rewards": -0.3743152916431427, "rewards/batch_rewards": -0.3389846682548523, "rewards/true_r_var": 0.6979203820228577, "rewards/a_var1": 3.163557767868042, "rewards/a_var2": 2.788938283920288, "rewards/r_var1": 2.166489601135254, "rewards/r_var2": 2.025930404663086, "rewards/r_q_v_max": 2.870990753173828, "rewards/r_q_v_min": -7.697368621826172, "rewards/r_true_max": 2.1849231719970703, "rewards/r_true_min": -3.30602765083313, "charts/SPS": 75.0, "losses/alpha_loss": -0.016960041597485542}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_200355-zwogpj4b/run-zwogpj4b.wandb b/algorithm/wandb/run-20250212_200355-zwogpj4b/run-zwogpj4b.wandb
deleted file mode 100644
index 1261e0a..0000000
Binary files a/algorithm/wandb/run-20250212_200355-zwogpj4b/run-zwogpj4b.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 1611b52..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v2"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/conda-environment.yaml b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/config.yaml b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/config.yaml
deleted file mode 100644
index 3c236ad..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739365848.12105
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v2
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/diff.patch b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/events.out.tfevents.1739365853.Tri.3922.0 b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/events.out.tfevents.1739365853.Tri.3922.0
deleted file mode 120000
index fc55246..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/events.out.tfevents.1739365853.Tri.3922.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v2__sac_rrd_iq__1__1739365844/events.out.tfevents.1739365853.Tri.3922.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/requirements.txt b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/wandb-metadata.json b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/wandb-metadata.json
deleted file mode 100644
index 8d3c170..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:10:49.650487",
-    "startedAt": "2025-02-12T13:10:48.114839",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.172794342041016
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/wandb-summary.json b/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/wandb-summary.json
deleted file mode 100644
index e21e531..0000000
--- a/algorithm/wandb/run-20250212_201048-iw4o0cxu/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 0, "_timestamp": 1739365853.0879092, "_runtime": 4.966859340667725, "_step": 0, "_wandb": {"runtime": 3}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201048-iw4o0cxu/run-iw4o0cxu.wandb b/algorithm/wandb/run-20250212_201048-iw4o0cxu/run-iw4o0cxu.wandb
deleted file mode 100644
index df8a4f8..0000000
Binary files a/algorithm/wandb/run-20250212_201048-iw4o0cxu/run-iw4o0cxu.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250212_201258-m24sohat/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 1611b52..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v2"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/conda-environment.yaml b/algorithm/wandb/run-20250212_201258-m24sohat/files/conda-environment.yaml
deleted file mode 100644
index 58d7c97..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/conda-environment.yaml
+++ /dev/null
@@ -1,124 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - mujoco-py==2.1.2.14
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/config.yaml b/algorithm/wandb/run-20250212_201258-m24sohat/files/config.yaml
deleted file mode 100644
index afe8404..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739365978.829331
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v2
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/diff.patch b/algorithm/wandb/run-20250212_201258-m24sohat/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/events.out.tfevents.1739365986.Tri.4881.0 b/algorithm/wandb/run-20250212_201258-m24sohat/files/events.out.tfevents.1739365986.Tri.4881.0
deleted file mode 120000
index 2e0121d..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/events.out.tfevents.1739365986.Tri.4881.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v2__sac_rrd_iq__1__1739365974/events.out.tfevents.1739365986.Tri.4881.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/requirements.txt b/algorithm/wandb/run-20250212_201258-m24sohat/files/requirements.txt
deleted file mode 100644
index 688e5cb..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/requirements.txt
+++ /dev/null
@@ -1,110 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco-py==2.1.2.14
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/wandb-metadata.json b/algorithm/wandb/run-20250212_201258-m24sohat/files/wandb-metadata.json
deleted file mode 100644
index 3c95926..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:13:02.017335",
-    "startedAt": "2025-02-12T13:12:58.822777",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.188419342041016
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/files/wandb-summary.json b/algorithm/wandb/run-20250212_201258-m24sohat/files/wandb-summary.json
deleted file mode 100644
index c67cdd3..0000000
--- a/algorithm/wandb/run-20250212_201258-m24sohat/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 0, "_timestamp": 1739365986.17716, "_runtime": 7.347829103469849, "_step": 0, "_wandb": {"runtime": 7}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_201258-m24sohat/run-m24sohat.wandb b/algorithm/wandb/run-20250212_201258-m24sohat/run-m24sohat.wandb
deleted file mode 100644
index d99e8d3..0000000
Binary files a/algorithm/wandb/run-20250212_201258-m24sohat/run-m24sohat.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/code/algorithm/sac_rrd.py b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/code/algorithm/sac_rrd.py
deleted file mode 100644
index a8e565c..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/code/algorithm/sac_rrd.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/conda-environment.yaml b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/conda-environment.yaml
deleted file mode 100644
index 58d7c97..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/conda-environment.yaml
+++ /dev/null
@@ -1,124 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - mujoco-py==2.1.2.14
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/config.yaml b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/config.yaml
deleted file mode 100644
index 230593c..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739366783.257156
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/diff.patch b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/events.out.tfevents.1739366789.Tri.17650.0 b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/events.out.tfevents.1739366789.Tri.17650.0
deleted file mode 120000
index 1be460b..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/events.out.tfevents.1739366789.Tri.17650.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd__1__1739366779/events.out.tfevents.1739366789.Tri.17650.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/requirements.txt b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/requirements.txt
deleted file mode 100644
index 688e5cb..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/requirements.txt
+++ /dev/null
@@ -1,110 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco-py==2.1.2.14
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/wandb-metadata.json b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/wandb-metadata.json
deleted file mode 100644
index 9c648d9..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:26:25.997441",
-    "startedAt": "2025-02-12T13:26:23.248580",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd.py",
-    "codePath": "algorithm/sac_rrd.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.35690689086914
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/wandb-summary.json b/algorithm/wandb/run-20250212_202623-l8f8hofb/files/wandb-summary.json
deleted file mode 100644
index b3c4eb6..0000000
--- a/algorithm/wandb/run-20250212_202623-l8f8hofb/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 0, "_timestamp": 1739366789.8931851, "_runtime": 6.636029243469238, "_step": 0, "_wandb": {"runtime": 7}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202623-l8f8hofb/run-l8f8hofb.wandb b/algorithm/wandb/run-20250212_202623-l8f8hofb/run-l8f8hofb.wandb
deleted file mode 100644
index 3c5aa96..0000000
Binary files a/algorithm/wandb/run-20250212_202623-l8f8hofb/run-l8f8hofb.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/code/algorithm/sac_rrd.py b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/code/algorithm/sac_rrd.py
deleted file mode 100644
index a8e565c..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/code/algorithm/sac_rrd.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/conda-environment.yaml b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/config.yaml b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/config.yaml
deleted file mode 100644
index 0175219..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739366885.236942
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/diff.patch b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/events.out.tfevents.1739366890.Tri.18341.0 b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/events.out.tfevents.1739366890.Tri.18341.0
deleted file mode 120000
index f931644..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/events.out.tfevents.1739366890.Tri.18341.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd__1__1739366880/events.out.tfevents.1739366890.Tri.18341.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/requirements.txt b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/wandb-metadata.json b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/wandb-metadata.json
deleted file mode 100644
index 6a23159..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:28:06.695074",
-    "startedAt": "2025-02-12T13:28:05.230807",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd.py",
-    "codePath": "algorithm/sac_rrd.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.350841522216797
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/wandb-summary.json b/algorithm/wandb/run-20250212_202805-2a2wgtam/files/wandb-summary.json
deleted file mode 100644
index 0b19256..0000000
--- a/algorithm/wandb/run-20250212_202805-2a2wgtam/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 19978, "_timestamp": 1739367239.910189, "_runtime": 354.67324686050415, "_step": 574, "charts/episodic_return": 207.3153533935547, "charts/episodic_length": 107.0, "losses/qf1_values": 125.43731689453125, "losses/qf2_values": 125.64884185791016, "losses/qf1_loss": 79.93930053710938, "losses/qf2_loss": 76.53275299072266, "losses/qf_loss": 78.23602294921875, "losses/actor_loss": -131.45877075195312, "losses/alpha": 0.1505078673362732, "losses/re_loss": 0.0010176966898143291, "rewards/q_r_loss": 80.68196105957031, "rewards/pre_re": -0.246994286775589, "rewards/pre_rewards": 1.6941438913345337, "rewards/eps_rewards": 1.6620118618011475, "rewards/batch_rewards": 1.6891863346099854, "rewards/true_r_var": 0.16731750965118408, "rewards/a_var1": 77.56716918945312, "rewards/a_var2": 74.6839370727539, "rewards/r_var1": 77.0297622680664, "rewards/r_var2": 74.506591796875, "rewards/r_q_v_max": 68.45591735839844, "rewards/r_q_v_min": -16.232093811035156, "rewards/r_true_max": 2.146827459335327, "rewards/r_true_min": 0.9896955490112305, "charts/SPS": 57.0, "losses/alpha_loss": -0.18559712171554565}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_202805-2a2wgtam/run-2a2wgtam.wandb b/algorithm/wandb/run-20250212_202805-2a2wgtam/run-2a2wgtam.wandb
deleted file mode 100644
index eb302e0..0000000
Binary files a/algorithm/wandb/run-20250212_202805-2a2wgtam/run-2a2wgtam.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index e1d633f..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/conda-environment.yaml b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/config.yaml b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/config.yaml
deleted file mode 100644
index 36d57af..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739367022.266596
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/diff.patch b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/events.out.tfevents.1739367026.Tri.20241.0 b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/events.out.tfevents.1739367026.Tri.20241.0
deleted file mode 120000
index 37d5e68..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/events.out.tfevents.1739367026.Tri.20241.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739367019/events.out.tfevents.1739367026.Tri.20241.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/requirements.txt b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/wandb-metadata.json b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/wandb-metadata.json
deleted file mode 100644
index 12ef41b..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:30:23.756206",
-    "startedAt": "2025-02-12T13:30:22.261707",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.35199737548828
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/wandb-summary.json b/algorithm/wandb/run-20250212_203022-o9zauuf6/files/wandb-summary.json
deleted file mode 100644
index 930c002..0000000
--- a/algorithm/wandb/run-20250212_203022-o9zauuf6/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 132700, "_timestamp": 1739370550.8914173, "_runtime": 3528.6248211860657, "_step": 1489, "charts/episodic_return": 476.96514892578125, "charts/episodic_length": 1000.0, "losses/qf1_values": 36.301700592041016, "losses/qf2_values": 36.66102600097656, "losses/qf1_loss": 0.05972418189048767, "losses/qf2_loss": 0.03348996490240097, "losses/qf_loss": 0.04660707339644432, "losses/actor_loss": -36.17685317993164, "losses/alpha": 0.016080399975180626, "losses/re_loss": 0.0019449560204520822, "rewards/q_r_loss": 0.05611637979745865, "rewards/pre_re": 0.32510513067245483, "rewards/pre_rewards": 0.5044325590133667, "rewards/eps_rewards": 0.5440206527709961, "rewards/batch_rewards": 0.5377411246299744, "rewards/true_r_var": 0.04789115488529205, "rewards/a_var1": 0.04019533097743988, "rewards/a_var2": 0.04767309129238129, "rewards/r_var1": 0.01180017739534378, "rewards/r_var2": 0.013775750063359737, "rewards/r_q_v_max": 0.6156349182128906, "rewards/r_q_v_min": -0.5383129119873047, "rewards/r_true_max": 1.5279239416122437, "rewards/r_true_min": -1.8114103078842163, "charts/SPS": 37.0, "losses/alpha_loss": 0.05812598392367363}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203022-o9zauuf6/run-o9zauuf6.wandb b/algorithm/wandb/run-20250212_203022-o9zauuf6/run-o9zauuf6.wandb
deleted file mode 100644
index 98fdb88..0000000
Binary files a/algorithm/wandb/run-20250212_203022-o9zauuf6/run-o9zauuf6.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/code/algorithm/sac_rrd.py b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/code/algorithm/sac_rrd.py
deleted file mode 100644
index a8e565c..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/code/algorithm/sac_rrd.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/conda-environment.yaml b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/config.yaml b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/config.yaml
deleted file mode 100644
index 4d92e0d..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739367055.67879
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/diff.patch b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/events.out.tfevents.1739367061.Tri.20821.0 b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/events.out.tfevents.1739367061.Tri.20821.0
deleted file mode 120000
index 3e07964..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/events.out.tfevents.1739367061.Tri.20821.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd__1__1739367053/events.out.tfevents.1739367061.Tri.20821.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/requirements.txt b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/wandb-metadata.json b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/wandb-metadata.json
deleted file mode 100644
index a5e552f..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:30:56.959709",
-    "startedAt": "2025-02-12T13:30:55.673143",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd.py",
-    "codePath": "algorithm/sac_rrd.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.352367401123047
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/wandb-summary.json b/algorithm/wandb/run-20250212_203055-s1mce6hd/files/wandb-summary.json
deleted file mode 100644
index 3e76b8d..0000000
--- a/algorithm/wandb/run-20250212_203055-s1mce6hd/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 138940, "_timestamp": 1739370552.7790742, "_runtime": 3497.100284099579, "_step": 2521, "charts/episodic_return": 481.00750732421875, "charts/episodic_length": 161.0, "losses/qf1_values": 163.55931091308594, "losses/qf2_values": 162.00115966796875, "losses/qf1_loss": 6.075099945068359, "losses/qf2_loss": 10.161399841308594, "losses/qf_loss": 8.118249893188477, "losses/actor_loss": -163.72975158691406, "losses/alpha": 0.08660666644573212, "losses/re_loss": 1.9075996533501893e-06, "rewards/q_r_loss": 10.162818908691406, "rewards/pre_re": 0.7631360292434692, "rewards/pre_rewards": 2.57326602935791, "rewards/eps_rewards": 2.571408987045288, "rewards/batch_rewards": 2.5627331733703613, "rewards/true_r_var": 0.6314367055892944, "rewards/a_var1": 6.062943458557129, "rewards/a_var2": 7.941348552703857, "rewards/r_var1": 6.6999053955078125, "rewards/r_var2": 8.323409080505371, "rewards/r_q_v_max": 11.009456634521484, "rewards/r_q_v_min": -15.972549438476562, "rewards/r_true_max": 3.823664903640747, "rewards/r_true_min": 1.0058754682540894, "charts/SPS": 39.0, "losses/alpha_loss": 0.04974275827407837}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_203055-s1mce6hd/run-s1mce6hd.wandb b/algorithm/wandb/run-20250212_203055-s1mce6hd/run-s1mce6hd.wandb
deleted file mode 100644
index 9117d77..0000000
Binary files a/algorithm/wandb/run-20250212_203055-s1mce6hd/run-s1mce6hd.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250212_204644-owvgx380/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index e1d633f..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/conda-environment.yaml b/algorithm/wandb/run-20250212_204644-owvgx380/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/config.yaml b/algorithm/wandb/run-20250212_204644-owvgx380/files/config.yaml
deleted file mode 100644
index 6e59660..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739368004.117166
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/diff.patch b/algorithm/wandb/run-20250212_204644-owvgx380/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/events.out.tfevents.1739368008.Tri.30131.0 b/algorithm/wandb/run-20250212_204644-owvgx380/files/events.out.tfevents.1739368008.Tri.30131.0
deleted file mode 120000
index cbd2725..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/events.out.tfevents.1739368008.Tri.30131.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739368001/events.out.tfevents.1739368008.Tri.30131.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/requirements.txt b/algorithm/wandb/run-20250212_204644-owvgx380/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/wandb-metadata.json b/algorithm/wandb/run-20250212_204644-owvgx380/files/wandb-metadata.json
deleted file mode 100644
index 7b6632f..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:46:45.835580",
-    "startedAt": "2025-02-12T13:46:44.112353",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.357791900634766
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/files/wandb-summary.json b/algorithm/wandb/run-20250212_204644-owvgx380/files/wandb-summary.json
deleted file mode 100644
index 6446295..0000000
--- a/algorithm/wandb/run-20250212_204644-owvgx380/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 90561, "_timestamp": 1739370554.6958392, "_runtime": 2550.5786731243134, "_step": 1019, "charts/episodic_return": 535.5045166015625, "charts/episodic_length": 1000.0, "losses/qf1_values": 29.19015121459961, "losses/qf2_values": 29.441940307617188, "losses/qf1_loss": 0.1284385323524475, "losses/qf2_loss": 0.11768889427185059, "losses/qf_loss": 0.12306371331214905, "losses/actor_loss": -29.28022003173828, "losses/alpha": 0.01384433638304472, "losses/re_loss": 9.424514502143211e-08, "rewards/q_r_loss": 0.14649289846420288, "rewards/pre_re": 0.36444827914237976, "rewards/pre_rewards": 0.3740706145763397, "rewards/eps_rewards": 0.3737022578716278, "rewards/batch_rewards": 0.3744090795516968, "rewards/true_r_var": 0.08159977197647095, "rewards/a_var1": 0.27356910705566406, "rewards/a_var2": 0.1190287321805954, "rewards/r_var1": 0.12843699753284454, "rewards/r_var2": 0.05491440370678902, "rewards/r_q_v_max": 2.4344520568847656, "rewards/r_q_v_min": -2.349498748779297, "rewards/r_true_max": 2.0684492588043213, "rewards/r_true_min": -1.2322379350662231, "charts/SPS": 35.0, "losses/alpha_loss": 0.03421461582183838}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204644-owvgx380/run-owvgx380.wandb b/algorithm/wandb/run-20250212_204644-owvgx380/run-owvgx380.wandb
deleted file mode 100644
index 973d64c..0000000
Binary files a/algorithm/wandb/run-20250212_204644-owvgx380/run-owvgx380.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250212_204703-ye84f8va/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index e1d633f..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/conda-environment.yaml b/algorithm/wandb/run-20250212_204703-ye84f8va/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/config.yaml b/algorithm/wandb/run-20250212_204703-ye84f8va/files/config.yaml
deleted file mode 100644
index c325bc1..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739368023.269561
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/diff.patch b/algorithm/wandb/run-20250212_204703-ye84f8va/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/events.out.tfevents.1739368028.Tri.30510.0 b/algorithm/wandb/run-20250212_204703-ye84f8va/files/events.out.tfevents.1739368028.Tri.30510.0
deleted file mode 120000
index 28728d5..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/events.out.tfevents.1739368028.Tri.30510.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739368020/events.out.tfevents.1739368028.Tri.30510.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/requirements.txt b/algorithm/wandb/run-20250212_204703-ye84f8va/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/wandb-metadata.json b/algorithm/wandb/run-20250212_204703-ye84f8va/files/wandb-metadata.json
deleted file mode 100644
index b298e0d..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-12T13:47:05.657915",
-    "startedAt": "2025-02-12T13:47:03.262776",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.008999999999,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.009,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.35810089111328
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/files/wandb-summary.json b/algorithm/wandb/run-20250212_204703-ye84f8va/files/wandb-summary.json
deleted file mode 100644
index c340bcc..0000000
--- a/algorithm/wandb/run-20250212_204703-ye84f8va/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 89561, "_timestamp": 1739370554.098337, "_runtime": 2530.828775882721, "_step": 1008, "charts/episodic_return": -15.088252067565918, "charts/episodic_length": 12.0, "losses/qf1_values": 28.649028778076172, "losses/qf2_values": 28.018213272094727, "losses/qf1_loss": 0.030823299661278725, "losses/qf2_loss": 0.4083421528339386, "losses/qf_loss": 0.2195827215909958, "losses/actor_loss": -28.185352325439453, "losses/alpha": 0.014986280351877213, "losses/re_loss": 0.00013410170504357666, "rewards/q_r_loss": 0.4713699519634247, "rewards/pre_re": -0.2707933783531189, "rewards/pre_rewards": 0.356606125831604, "rewards/eps_rewards": 0.3458997905254364, "rewards/batch_rewards": 0.3430804908275604, "rewards/true_r_var": 0.06139156222343445, "rewards/a_var1": 0.08623471111059189, "rewards/a_var2": 0.08698219805955887, "rewards/r_var1": 0.03057897835969925, "rewards/r_var2": 0.029889408499002457, "rewards/r_q_v_max": 0.4855232238769531, "rewards/r_q_v_min": -0.8269824981689453, "rewards/r_true_max": 0.7939835786819458, "rewards/r_true_min": -1.2437642812728882, "charts/SPS": 35.0, "losses/alpha_loss": 0.04056593030691147}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250212_204703-ye84f8va/run-ye84f8va.wandb b/algorithm/wandb/run-20250212_204703-ye84f8va/run-ye84f8va.wandb
deleted file mode 100644
index 2e1319a..0000000
Binary files a/algorithm/wandb/run-20250212_204703-ye84f8va/run-ye84f8va.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index e1d633f..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/conda-environment.yaml b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/config.yaml b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/config.yaml
deleted file mode 100644
index 3e8920f..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739454077.418599
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/diff.patch b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/events.out.tfevents.1739454085.Tri.1096.0 b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/events.out.tfevents.1739454085.Tri.1096.0
deleted file mode 120000
index 8058a57..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/events.out.tfevents.1739454085.Tri.1096.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739454071/events.out.tfevents.1739454085.Tri.1096.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/requirements.txt b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/wandb-metadata.json b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/wandb-metadata.json
deleted file mode 100644
index 9d4dbdd..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-13T13:41:21.669415",
-    "startedAt": "2025-02-13T13:41:17.402816",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.416259765625
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/wandb-summary.json b/algorithm/wandb/run-20250213_204117-0mpy96zg/files/wandb-summary.json
deleted file mode 100644
index dab1b85..0000000
--- a/algorithm/wandb/run-20250213_204117-0mpy96zg/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2750200, "_timestamp": 1739513492.8800952, "_runtime": 59415.46149635315, "_step": 30360, "charts/episodic_return": 759.5783081054688, "charts/episodic_length": 1000.0, "losses/qf1_values": 76.6629867553711, "losses/qf2_values": 76.68693542480469, "losses/qf1_loss": 0.002804258605465293, "losses/qf2_loss": 0.003197197802364826, "losses/qf_loss": 0.0030007283203303814, "losses/actor_loss": -76.61465454101562, "losses/alpha": 0.00418313592672348, "losses/re_loss": 1.0752487469289917e-06, "rewards/q_r_loss": 0.022804157808423042, "rewards/pre_re": 0.7800708413124084, "rewards/pre_rewards": 0.7817447185516357, "rewards/eps_rewards": 0.7814702391624451, "rewards/batch_rewards": 0.7995957136154175, "rewards/true_r_var": 0.0894930362701416, "rewards/a_var1": 0.08324326574802399, "rewards/a_var2": 0.09086521714925766, "rewards/r_var1": 0.0027881998103111982, "rewards/r_var2": 0.002415846101939678, "rewards/r_q_v_max": 0.93609619140625, "rewards/r_q_v_min": 0.5287094116210938, "rewards/r_true_max": 1.315232276916504, "rewards/r_true_min": -2.1169440746307373, "charts/SPS": 46.0, "losses/alpha_loss": 0.00290177040733397}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204117-0mpy96zg/run-0mpy96zg.wandb b/algorithm/wandb/run-20250213_204117-0mpy96zg/run-0mpy96zg.wandb
deleted file mode 100644
index 83bbae6..0000000
Binary files a/algorithm/wandb/run-20250213_204117-0mpy96zg/run-0mpy96zg.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/code/algorithm/sac_rrd.py b/algorithm/wandb/run-20250213_204144-br459tjk/files/code/algorithm/sac_rrd.py
deleted file mode 100644
index a8e565c..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/code/algorithm/sac_rrd.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Hopper-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/conda-environment.yaml b/algorithm/wandb/run-20250213_204144-br459tjk/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/config.yaml b/algorithm/wandb/run-20250213_204144-br459tjk/files/config.yaml
deleted file mode 100644
index ce2b9b4..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739454104.03581
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Hopper-v4
-exp_name:
-  desc: null
-  value: sac_rrd
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/diff.patch b/algorithm/wandb/run-20250213_204144-br459tjk/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/events.out.tfevents.1739454107.Tri.1383.0 b/algorithm/wandb/run-20250213_204144-br459tjk/files/events.out.tfevents.1739454107.Tri.1383.0
deleted file mode 120000
index 57fef09..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/events.out.tfevents.1739454107.Tri.1383.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Hopper-v4__sac_rrd__1__1739454100/events.out.tfevents.1739454107.Tri.1383.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/requirements.txt b/algorithm/wandb/run-20250213_204144-br459tjk/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/wandb-metadata.json b/algorithm/wandb/run-20250213_204144-br459tjk/files/wandb-metadata.json
deleted file mode 100644
index 4b0ec68..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-13T13:41:45.162942",
-    "startedAt": "2025-02-13T13:41:44.029748",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd.py",
-    "codePath": "algorithm/sac_rrd.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.416454315185547
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/files/wandb-summary.json b/algorithm/wandb/run-20250213_204144-br459tjk/files/wandb-summary.json
deleted file mode 100644
index bd419eb..0000000
--- a/algorithm/wandb/run-20250213_204144-br459tjk/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2300700, "_timestamp": 1739499572.1556742, "_runtime": 45468.11986422539, "_step": 28082, "charts/episodic_return": 3316.80078125, "charts/episodic_length": 1000.0, "losses/qf1_values": 288.45220947265625, "losses/qf2_values": 287.72509765625, "losses/qf1_loss": 12.200681686401367, "losses/qf2_loss": 9.163495063781738, "losses/qf_loss": 10.682088851928711, "losses/actor_loss": -287.0212707519531, "losses/alpha": 0.10258321464061737, "losses/re_loss": 0.0006822142750024796, "rewards/q_r_loss": 7.726541519165039, "rewards/pre_re": 3.713881254196167, "rewards/pre_rewards": 3.6198859214782715, "rewards/eps_rewards": 3.595885753631592, "rewards/batch_rewards": 3.58925724029541, "rewards/true_r_var": 0.3182542026042938, "rewards/a_var1": 9.742842674255371, "rewards/a_var2": 8.376380920410156, "rewards/r_var1": 9.42934513092041, "rewards/r_var2": 8.405497550964355, "rewards/r_q_v_max": 9.47796630859375, "rewards/r_q_v_min": -12.30950927734375, "rewards/r_true_max": 4.524238109588623, "rewards/r_true_min": 1.0382429361343384, "charts/SPS": 50.0, "losses/alpha_loss": 0.12019819766283035}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250213_204144-br459tjk/run-br459tjk.wandb b/algorithm/wandb/run-20250213_204144-br459tjk/run-br459tjk.wandb
deleted file mode 100644
index 15fe5b4..0000000
Binary files a/algorithm/wandb/run-20250213_204144-br459tjk/run-br459tjk.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/code/algorithm/sac_rrd.py b/algorithm/wandb/run-20250214_041539-zvonu5np/files/code/algorithm/sac_rrd.py
deleted file mode 100644
index 8277e70..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/code/algorithm/sac_rrd.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "AntAnt-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/conda-environment.yaml b/algorithm/wandb/run-20250214_041539-zvonu5np/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/config.yaml b/algorithm/wandb/run-20250214_041539-zvonu5np/files/config.yaml
deleted file mode 100644
index 7aa6722..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739481339.80821
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: AntAnt-v4
-exp_name:
-  desc: null
-  value: sac_rrd
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/diff.patch b/algorithm/wandb/run-20250214_041539-zvonu5np/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/events.out.tfevents.1739481345.Tri.158665.0 b/algorithm/wandb/run-20250214_041539-zvonu5np/files/events.out.tfevents.1739481345.Tri.158665.0
deleted file mode 120000
index f323593..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/events.out.tfevents.1739481345.Tri.158665.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/AntAnt-v4__sac_rrd__1__1739481336/events.out.tfevents.1739481345.Tri.158665.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/requirements.txt b/algorithm/wandb/run-20250214_041539-zvonu5np/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/wandb-metadata.json b/algorithm/wandb/run-20250214_041539-zvonu5np/files/wandb-metadata.json
deleted file mode 100644
index f7e4122..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-13T21:15:41.053678",
-    "startedAt": "2025-02-13T21:15:39.797920",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd.py",
-    "codePath": "algorithm/sac_rrd.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.521923065185547
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/files/wandb-summary.json b/algorithm/wandb/run-20250214_041539-zvonu5np/files/wandb-summary.json
deleted file mode 100644
index b2b56e0..0000000
--- a/algorithm/wandb/run-20250214_041539-zvonu5np/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 0, "_timestamp": 1739481345.6656215, "_runtime": 5.857411623001099, "_step": 0, "_wandb": {"runtime": 4}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_041539-zvonu5np/run-zvonu5np.wandb b/algorithm/wandb/run-20250214_041539-zvonu5np/run-zvonu5np.wandb
deleted file mode 100644
index bd9e7e2..0000000
Binary files a/algorithm/wandb/run-20250214_041539-zvonu5np/run-zvonu5np.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/code/algorithm/sac_rrd.py b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/code/algorithm/sac_rrd.py
deleted file mode 100644
index e71246b..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/code/algorithm/sac_rrd.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/conda-environment.yaml b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/config.yaml b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/config.yaml
deleted file mode 100644
index 2241786..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739482134.683172
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/diff.patch b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/events.out.tfevents.1739482139.Tri.163302.0 b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/events.out.tfevents.1739482139.Tri.163302.0
deleted file mode 120000
index c0f50cf..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/events.out.tfevents.1739482139.Tri.163302.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd__1__1739482131/events.out.tfevents.1739482139.Tri.163302.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/requirements.txt b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/wandb-metadata.json b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/wandb-metadata.json
deleted file mode 100644
index 059b4a6..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-13T21:28:55.889242",
-    "startedAt": "2025-02-13T21:28:54.676154",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd.py",
-    "codePath": "algorithm/sac_rrd.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.525035858154297
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/wandb-summary.json b/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/wandb-summary.json
deleted file mode 100644
index 4ff3cb7..0000000
--- a/algorithm/wandb/run-20250214_042854-1o4p1s9o/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 1346600, "_timestamp": 1739514444.54724, "_runtime": 32309.86406803131, "_step": 15010, "charts/episodic_return": 257.0475158691406, "charts/episodic_length": 1000.0, "losses/qf1_values": 666.9969482421875, "losses/qf2_values": 666.1671752929688, "losses/qf1_loss": 23.840511322021484, "losses/qf2_loss": 24.632293701171875, "losses/qf_loss": 24.23640251159668, "losses/actor_loss": -667.1513671875, "losses/alpha": 0.07858144491910934, "losses/re_loss": 9.819604019867256e-05, "rewards/q_r_loss": 25.66838836669922, "rewards/pre_re": 1.077714443206787, "rewards/pre_rewards": 2.991987705230713, "rewards/eps_rewards": 3.006244421005249, "rewards/batch_rewards": 2.9958648681640625, "rewards/true_r_var": 1.2238589525222778, "rewards/a_var1": 23.863365173339844, "rewards/a_var2": 23.91854476928711, "rewards/r_var1": 23.84078598022461, "rewards/r_var2": 23.97745132446289, "rewards/r_q_v_max": 19.20880126953125, "rewards/r_q_v_min": -18.10601806640625, "rewards/r_true_max": 5.200700759887695, "rewards/r_true_min": -0.6600876450538635, "charts/SPS": 41.0, "losses/alpha_loss": 0.18967893719673157}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_042854-1o4p1s9o/run-1o4p1s9o.wandb b/algorithm/wandb/run-20250214_042854-1o4p1s9o/run-1o4p1s9o.wandb
deleted file mode 100644
index b2a06f7..0000000
Binary files a/algorithm/wandb/run-20250214_042854-1o4p1s9o/run-1o4p1s9o.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 3027718..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0.5
-
-    r_coefficient: float = 0.5
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/conda-environment.yaml b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/config.yaml b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/config.yaml
deleted file mode 100644
index fe756e8..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739499741.902096
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0.5
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.5
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/diff.patch b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/events.out.tfevents.1739499749.Tri.263022.0 b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/events.out.tfevents.1739499749.Tri.263022.0
deleted file mode 120000
index 334ba94..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/events.out.tfevents.1739499749.Tri.263022.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739499737/events.out.tfevents.1739499749.Tri.263022.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/requirements.txt b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/wandb-metadata.json b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/wandb-metadata.json
deleted file mode 100644
index ec5ece8..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T02:22:23.126479",
-    "startedAt": "2025-02-14T02:22:21.887277",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.60577392578125
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/wandb-summary.json b/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/wandb-summary.json
deleted file mode 100644
index b56c81f..0000000
--- a/algorithm/wandb/run-20250214_092221-b3e2g9hl/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 566500, "_timestamp": 1739514443.0889966, "_runtime": 14701.186900615692, "_step": 6290, "charts/episodic_return": 754.8685302734375, "charts/episodic_length": 1000.0, "losses/qf1_values": 40.44514465332031, "losses/qf2_values": 40.715599060058594, "losses/qf1_loss": 0.16687841713428497, "losses/qf2_loss": 0.285754531621933, "losses/qf_loss": 0.22631648182868958, "losses/actor_loss": -40.43457794189453, "losses/alpha": 0.007940752431750298, "losses/re_loss": 5.018410774937365e-06, "rewards/q_r_loss": 0.07557530701160431, "rewards/pre_re": 0.4659480154514313, "rewards/pre_rewards": 0.6876779198646545, "rewards/eps_rewards": 0.6857744455337524, "rewards/batch_rewards": 0.6922546625137329, "rewards/true_r_var": 0.04474932700395584, "rewards/a_var1": 0.058893244713544846, "rewards/a_var2": 0.06070138141512871, "rewards/r_var1": 0.010007822886109352, "rewards/r_var2": 0.011673522181808949, "rewards/r_q_v_max": 1.2245597839355469, "rewards/r_q_v_min": 0.09453582763671875, "rewards/r_true_max": 1.1465579271316528, "rewards/r_true_min": -0.41540950536727905, "charts/SPS": 38.0, "losses/alpha_loss": 0.011935424990952015}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_092221-b3e2g9hl/run-b3e2g9hl.wandb b/algorithm/wandb/run-20250214_092221-b3e2g9hl/run-b3e2g9hl.wandb
deleted file mode 100644
index 79a6883..0000000
Binary files a/algorithm/wandb/run-20250214_092221-b3e2g9hl/run-b3e2g9hl.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 41f5b5a..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 0
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/conda-environment.yaml b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/config.yaml b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/config.yaml
deleted file mode 100644
index fb50490..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739511774.841669
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/diff.patch b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/events.out.tfevents.1739511782.Tri.329592.0 b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/events.out.tfevents.1739511782.Tri.329592.0
deleted file mode 120000
index c0e9a37..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/events.out.tfevents.1739511782.Tri.329592.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739511770/events.out.tfevents.1739511782.Tri.329592.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/requirements.txt b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/wandb-metadata.json b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/wandb-metadata.json
deleted file mode 100644
index a58437e..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T05:42:56.256462",
-    "startedAt": "2025-02-14T05:42:54.823411",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.66259002685547
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/wandb-summary.json b/algorithm/wandb/run-20250214_124254-omn6s1ka/files/wandb-summary.json
deleted file mode 100644
index 5a8180b..0000000
--- a/algorithm/wandb/run-20250214_124254-omn6s1ka/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 51100, "_timestamp": 1739513363.1894479, "_runtime": 1588.3477787971497, "_step": 647, "charts/episodic_return": -143.06576538085938, "charts/episodic_length": 139.0, "losses/qf1_values": -36.3531494140625, "losses/qf2_values": -35.6617546081543, "losses/qf1_loss": 0.9961987137794495, "losses/qf2_loss": 0.09406422823667526, "losses/qf_loss": 0.5451314449310303, "losses/actor_loss": 36.021453857421875, "losses/alpha": 0.026747358962893486, "losses/re_loss": 1.2779436247001286e-06, "rewards/q_r_loss": 436.0200500488281, "rewards/pre_re": -4.739327907562256, "rewards/pre_rewards": -0.8939370512962341, "rewards/eps_rewards": -0.8886555433273315, "rewards/batch_rewards": -0.8665493726730347, "rewards/true_r_var": 1.0479769706726074, "rewards/a_var1": 431.4869079589844, "rewards/a_var2": 374.46697998046875, "rewards/r_var1": 434.07073974609375, "rewards/r_var2": 376.52899169921875, "rewards/r_q_v_max": 52.1284065246582, "rewards/r_q_v_min": -55.354339599609375, "rewards/r_true_max": 0.9540414810180664, "rewards/r_true_min": -3.097431182861328, "charts/SPS": 32.0, "losses/alpha_loss": -0.013550064526498318}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_124254-omn6s1ka/run-omn6s1ka.wandb b/algorithm/wandb/run-20250214_124254-omn6s1ka/run-omn6s1ka.wandb
deleted file mode 100644
index f002272..0000000
Binary files a/algorithm/wandb/run-20250214_124254-omn6s1ka/run-omn6s1ka.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_130947-38i6laor/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index b2addd2..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 0.2
-
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            # qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
-            # qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/conda-environment.yaml b/algorithm/wandb/run-20250214_130947-38i6laor/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/config.yaml b/algorithm/wandb/run-20250214_130947-38i6laor/files/config.yaml
deleted file mode 100644
index 6d2d461..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739513387.824575
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.2
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/diff.patch b/algorithm/wandb/run-20250214_130947-38i6laor/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/events.out.tfevents.1739513397.Tri.339485.0 b/algorithm/wandb/run-20250214_130947-38i6laor/files/events.out.tfevents.1739513397.Tri.339485.0
deleted file mode 120000
index 1a39c0c..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/events.out.tfevents.1739513397.Tri.339485.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739513382/events.out.tfevents.1739513397.Tri.339485.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/requirements.txt b/algorithm/wandb/run-20250214_130947-38i6laor/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/wandb-metadata.json b/algorithm/wandb/run-20250214_130947-38i6laor/files/wandb-metadata.json
deleted file mode 100644
index 0b8ca02..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T06:09:49.468462",
-    "startedAt": "2025-02-14T06:09:47.808482",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.67055892944336
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/files/wandb-summary.json b/algorithm/wandb/run-20250214_130947-38i6laor/files/wandb-summary.json
deleted file mode 100644
index 64396ff..0000000
--- a/algorithm/wandb/run-20250214_130947-38i6laor/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 33000, "_timestamp": 1739514442.8092213, "_runtime": 1054.984646320343, "_step": 409, "charts/episodic_return": 99.37588500976562, "charts/episodic_length": 112.0, "losses/qf1_values": 10.788015365600586, "losses/qf2_values": 10.276339530944824, "losses/qf1_loss": 0.6269376277923584, "losses/qf2_loss": 1.4065240621566772, "losses/qf_loss": 1.016730785369873, "losses/actor_loss": -11.508817672729492, "losses/alpha": 0.010140035301446915, "losses/re_loss": 0.003274262882769108, "rewards/q_r_loss": 3.0712428092956543, "rewards/pre_re": -1.7064576148986816, "rewards/pre_rewards": -0.5415570735931396, "rewards/eps_rewards": -0.48192620277404785, "rewards/batch_rewards": -0.5120732188224792, "rewards/true_r_var": 0.7793641686439514, "rewards/a_var1": 2.220810651779175, "rewards/a_var2": 2.0707807540893555, "rewards/r_var1": 1.800399899482727, "rewards/r_var2": 1.7460442781448364, "rewards/r_q_v_max": 2.006195068359375, "rewards/r_q_v_min": -5.396465301513672, "rewards/r_true_max": 1.1974703073501587, "rewards/r_true_min": -2.709773302078247, "charts/SPS": 34.0, "losses/alpha_loss": 0.001384790288284421}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_130947-38i6laor/run-38i6laor.wandb b/algorithm/wandb/run-20250214_130947-38i6laor/run-38i6laor.wandb
deleted file mode 100644
index 6bd2ca6..0000000
Binary files a/algorithm/wandb/run-20250214_130947-38i6laor/run-38i6laor.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index a114688..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,466 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 0.1
-
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/conda-environment.yaml b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/config.yaml b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/config.yaml
deleted file mode 100644
index 8a3b9db..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739516249.742759
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/diff.patch b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/events.out.tfevents.1739516259.Tri.3418.0 b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/events.out.tfevents.1739516259.Tri.3418.0
deleted file mode 120000
index 75e648c..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/events.out.tfevents.1739516259.Tri.3418.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739516244/events.out.tfevents.1739516259.Tri.3418.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/requirements.txt b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/wandb-metadata.json b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/wandb-metadata.json
deleted file mode 100644
index df84eb8..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T06:57:36.128563",
-    "startedAt": "2025-02-14T06:57:29.730527",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.637371063232422
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/wandb-summary.json b/algorithm/wandb/run-20250214_135729-fue6fu0e/files/wandb-summary.json
deleted file mode 100644
index 9972048..0000000
--- a/algorithm/wandb/run-20250214_135729-fue6fu0e/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2056400, "_timestamp": 1739564096.0456376, "_runtime": 47846.30287861824, "_step": 22842, "charts/episodic_return": 1339.8818359375, "charts/episodic_length": 1000.0, "losses/qf1_values": 40.99909973144531, "losses/qf2_values": 41.36872863769531, "losses/qf1_loss": 3.710361957550049, "losses/qf2_loss": 3.1789565086364746, "losses/qf_loss": 3.4446592330932617, "losses/q_loss": 30.632076263427734, "losses/actor_loss": -43.49229431152344, "losses/alpha": 0.0323517844080925, "losses/re_loss": 0.000847084098495543, "rewards/q_r_loss": 32.23870086669922, "rewards/pre_re": -3.5505471229553223, "rewards/pre_rewards": -1.260871410369873, "rewards/eps_rewards": -1.23504638671875, "rewards/batch_rewards": -1.2723721265792847, "rewards/true_r_var": 1.2994130849838257, "rewards/a_var1": 31.120437622070312, "rewards/a_var2": 29.870681762695312, "rewards/r_var1": 32.304420471191406, "rewards/r_var2": 30.74542808532715, "rewards/r_q_v_max": 8.728094100952148, "rewards/r_q_v_min": -23.62088394165039, "rewards/r_true_max": 1.3327499628067017, "rewards/r_true_min": -5.2420973777771, "charts/SPS": 42.0, "losses/alpha_loss": -0.189511239528656}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_135729-fue6fu0e/run-fue6fu0e.wandb b/algorithm/wandb/run-20250214_135729-fue6fu0e/run-fue6fu0e.wandb
deleted file mode 100644
index 4a216c8..0000000
Binary files a/algorithm/wandb/run-20250214_135729-fue6fu0e/run-fue6fu0e.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 584feeb..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 0.5
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/conda-environment.yaml b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/config.yaml b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/config.yaml
deleted file mode 100644
index 7d3eeb9..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739516462.79285
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.2
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/diff.patch b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/events.out.tfevents.1739516470.Tri.4537.0 b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/events.out.tfevents.1739516470.Tri.4537.0
deleted file mode 120000
index 5fa2bf7..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/events.out.tfevents.1739516470.Tri.4537.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739516459/events.out.tfevents.1739516470.Tri.4537.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/requirements.txt b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/wandb-metadata.json b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/wandb-metadata.json
deleted file mode 100644
index 7424d9f..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T07:01:06.200639",
-    "startedAt": "2025-02-14T07:01:02.769079",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.637977600097656
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/wandb-summary.json b/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/wandb-summary.json
deleted file mode 100644
index 8aa5c57..0000000
--- a/algorithm/wandb/run-20250214_140102-mlyu0ek8/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2030800, "_timestamp": 1739564092.6209898, "_runtime": 47629.82813978195, "_step": 22530, "charts/episodic_return": 1509.6151123046875, "charts/episodic_length": 1000.0, "losses/qf1_values": 67.19922637939453, "losses/qf2_values": 68.72183990478516, "losses/qf1_loss": 0.7458332777023315, "losses/qf2_loss": 1.3341515064239502, "losses/qf_loss": 1.039992332458496, "losses/q_loss": 2.8596599102020264, "losses/actor_loss": -69.57820129394531, "losses/alpha": 0.019099248573184013, "losses/re_loss": 1.0951134754577652e-07, "rewards/q_r_loss": 2.325388193130493, "rewards/pre_re": -1.6328589916229248, "rewards/pre_rewards": -1.0051578283309937, "rewards/eps_rewards": -1.0047192573547363, "rewards/batch_rewards": -0.9986490607261658, "rewards/true_r_var": 0.599248468875885, "rewards/a_var1": 2.2700164318084717, "rewards/a_var2": 2.2378957271575928, "rewards/r_var1": 2.113980770111084, "rewards/r_var2": 2.1178014278411865, "rewards/r_q_v_max": 2.49822998046875, "rewards/r_q_v_min": -8.656883239746094, "rewards/r_true_max": 1.1225303411483765, "rewards/r_true_min": -2.2685699462890625, "charts/SPS": 42.0, "losses/alpha_loss": -0.09895011782646179, "_wandb": {"runtime": 47631}}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140102-mlyu0ek8/run-mlyu0ek8.wandb b/algorithm/wandb/run-20250214_140102-mlyu0ek8/run-mlyu0ek8.wandb
deleted file mode 100644
index e77ab9f..0000000
Binary files a/algorithm/wandb/run-20250214_140102-mlyu0ek8/run-mlyu0ek8.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_140147-khhne854/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 584feeb..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 0.5
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/conda-environment.yaml b/algorithm/wandb/run-20250214_140147-khhne854/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/config.yaml b/algorithm/wandb/run-20250214_140147-khhne854/files/config.yaml
deleted file mode 100644
index 7ba29b0..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739516507.017912
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.5
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/diff.patch b/algorithm/wandb/run-20250214_140147-khhne854/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/events.out.tfevents.1739516516.Tri.4998.0 b/algorithm/wandb/run-20250214_140147-khhne854/files/events.out.tfevents.1739516516.Tri.4998.0
deleted file mode 120000
index c678a23..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/events.out.tfevents.1739516516.Tri.4998.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739516498/events.out.tfevents.1739516516.Tri.4998.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/requirements.txt b/algorithm/wandb/run-20250214_140147-khhne854/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/wandb-metadata.json b/algorithm/wandb/run-20250214_140147-khhne854/files/wandb-metadata.json
deleted file mode 100644
index fd598e8..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T07:01:51.151406",
-    "startedAt": "2025-02-14T07:01:47.009756",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.63831329345703
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/files/wandb-summary.json b/algorithm/wandb/run-20250214_140147-khhne854/files/wandb-summary.json
deleted file mode 100644
index 4017783..0000000
--- a/algorithm/wandb/run-20250214_140147-khhne854/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 662200, "_timestamp": 1739537872.48723, "_runtime": 21365.469318151474, "_step": 7199, "charts/episodic_return": 504.8205261230469, "charts/episodic_length": 1000.0, "losses/qf1_values": 63.64276123046875, "losses/qf2_values": 63.95095443725586, "losses/qf1_loss": 0.604888916015625, "losses/qf2_loss": 0.41051948070526123, "losses/qf_loss": 0.5077041983604431, "losses/q_loss": 0.8644862771034241, "losses/actor_loss": -64.13156127929688, "losses/alpha": 0.03468220308423042, "losses/re_loss": 7.182419722084887e-06, "rewards/q_r_loss": 1.0833626985549927, "rewards/pre_re": 0.039822474122047424, "rewards/pre_rewards": 0.6643880605697632, "rewards/eps_rewards": 0.6646088361740112, "rewards/batch_rewards": 0.659328281879425, "rewards/true_r_var": 0.03735242784023285, "rewards/a_var1": 0.7118792533874512, "rewards/a_var2": 0.7256675958633423, "rewards/r_var1": 0.6842345595359802, "rewards/r_var2": 0.7374659776687622, "rewards/r_q_v_max": 3.50067138671875, "rewards/r_q_v_min": -4.647525787353516, "rewards/r_true_max": 1.0933903455734253, "rewards/r_true_min": -0.6437033414840698, "charts/SPS": 31.0, "losses/alpha_loss": 0.11261814832687378}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_140147-khhne854/run-khhne854.wandb b/algorithm/wandb/run-20250214_140147-khhne854/run-khhne854.wandb
deleted file mode 100644
index 4fab270..0000000
Binary files a/algorithm/wandb/run-20250214_140147-khhne854/run-khhne854.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 863893d..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 256
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 0.01
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/conda-environment.yaml b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/config.yaml b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/config.yaml
deleted file mode 100644
index d8eb6b5..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739518264.381357
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 256
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.01
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/diff.patch b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/events.out.tfevents.1739518271.Tri.14122.0 b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/events.out.tfevents.1739518271.Tri.14122.0
deleted file mode 120000
index 8316f7e..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/events.out.tfevents.1739518271.Tri.14122.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739518257/events.out.tfevents.1739518271.Tri.14122.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/requirements.txt b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/wandb-metadata.json b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/wandb-metadata.json
deleted file mode 100644
index c573181..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T07:31:07.885848",
-    "startedAt": "2025-02-14T07:31:04.361644",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.645790100097656
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/wandb-summary.json b/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/wandb-summary.json
deleted file mode 100644
index 45c9ab6..0000000
--- a/algorithm/wandb/run-20250214_143104-j6rhqbgx/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 76900, "_timestamp": 1739520601.68431, "_runtime": 2337.302953004837, "_step": 1025, "charts/episodic_return": 69.269775390625, "charts/episodic_length": 200.0, "losses/qf1_values": -13.939764022827148, "losses/qf2_values": -14.261383056640625, "losses/qf1_loss": 0.7660423517227173, "losses/qf2_loss": 0.6712401509284973, "losses/qf_loss": 0.7186412811279297, "losses/q_loss": 64.09925842285156, "losses/actor_loss": 13.835437774658203, "losses/alpha": 0.011915713548660278, "losses/re_loss": 0.00017808274424169213, "rewards/q_r_loss": 63.07286834716797, "rewards/pre_re": -1.4262062311172485, "rewards/pre_rewards": -0.4883376955986023, "rewards/eps_rewards": -0.48192620277404785, "rewards/batch_rewards": -0.4431503117084503, "rewards/true_r_var": 0.7727712988853455, "rewards/a_var1": 60.903202056884766, "rewards/a_var2": 67.64796447753906, "rewards/r_var1": 58.90430450439453, "rewards/r_var2": 66.1420669555664, "rewards/r_q_v_max": 17.72615623474121, "rewards/r_q_v_min": -29.70217514038086, "rewards/r_true_max": 1.1974703073501587, "rewards/r_true_min": -2.709773302078247, "charts/SPS": 33.0, "losses/alpha_loss": 0.0024897363036870956}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_143104-j6rhqbgx/run-j6rhqbgx.wandb b/algorithm/wandb/run-20250214_143104-j6rhqbgx/run-j6rhqbgx.wandb
deleted file mode 100644
index 8a7cea4..0000000
Binary files a/algorithm/wandb/run-20250214_143104-j6rhqbgx/run-j6rhqbgx.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250214_151037-23fejtfj/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 095fd00..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 512
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 0.3
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/conda-environment.yaml b/algorithm/wandb/run-20250214_151037-23fejtfj/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/config.yaml b/algorithm/wandb/run-20250214_151037-23fejtfj/files/config.yaml
deleted file mode 100644
index 46c8e6e..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739520637.940097
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.3
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/diff.patch b/algorithm/wandb/run-20250214_151037-23fejtfj/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/events.out.tfevents.1739520641.Tri.26650.0 b/algorithm/wandb/run-20250214_151037-23fejtfj/files/events.out.tfevents.1739520641.Tri.26650.0
deleted file mode 120000
index 9446dde..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/events.out.tfevents.1739520641.Tri.26650.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739520633/events.out.tfevents.1739520641.Tri.26650.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/requirements.txt b/algorithm/wandb/run-20250214_151037-23fejtfj/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/wandb-metadata.json b/algorithm/wandb/run-20250214_151037-23fejtfj/files/wandb-metadata.json
deleted file mode 100644
index 8704b43..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-14T08:10:39.259036",
-    "startedAt": "2025-02-14T08:10:37.918625",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.65764617919922
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/files/wandb-summary.json b/algorithm/wandb/run-20250214_151037-23fejtfj/files/wandb-summary.json
deleted file mode 100644
index 96da08c..0000000
--- a/algorithm/wandb/run-20250214_151037-23fejtfj/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 511800, "_timestamp": 1739537861.196214, "_runtime": 17223.256116867065, "_step": 5724, "charts/episodic_return": 636.3627319335938, "charts/episodic_length": 1000.0, "losses/qf1_values": 50.260929107666016, "losses/qf2_values": 49.920135498046875, "losses/qf1_loss": 0.1485319435596466, "losses/qf2_loss": 0.017834262922406197, "losses/qf_loss": 0.08318310230970383, "losses/q_loss": 0.17048707604408264, "losses/actor_loss": -50.0525016784668, "losses/alpha": 0.011056393384933472, "losses/re_loss": 7.2022412496153265e-06, "rewards/q_r_loss": 0.06142505258321762, "rewards/pre_re": 0.5271181464195251, "rewards/pre_rewards": 0.5153272151947021, "rewards/eps_rewards": 0.5109499096870422, "rewards/batch_rewards": 0.5131518244743347, "rewards/true_r_var": 0.04756493121385574, "rewards/a_var1": 0.14335528016090393, "rewards/a_var2": 0.07185227423906326, "rewards/r_var1": 0.06992447376251221, "rewards/r_var2": 0.058556027710437775, "rewards/r_q_v_max": 1.4862213134765625, "rewards/r_q_v_min": -2.4035682678222656, "rewards/r_true_max": 1.4338358640670776, "rewards/r_true_min": -1.2129489183425903, "charts/SPS": 29.0, "losses/alpha_loss": 0.024079520255327225}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250214_151037-23fejtfj/run-23fejtfj.wandb b/algorithm/wandb/run-20250214_151037-23fejtfj/run-23fejtfj.wandb
deleted file mode 100644
index ea5cd14..0000000
Binary files a/algorithm/wandb/run-20250214_151037-23fejtfj/run-23fejtfj.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250216_222311-q54yn29f/files/config.yaml b/algorithm/wandb/run-20250216_222311-q54yn29f/files/config.yaml
deleted file mode 100644
index 173f533..0000000
--- a/algorithm/wandb/run-20250216_222311-q54yn29f/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739719391.327388
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250216_222311-q54yn29f/run-q54yn29f.wandb b/algorithm/wandb/run-20250216_222311-q54yn29f/run-q54yn29f.wandb
deleted file mode 100644
index e0cb33e..0000000
Binary files a/algorithm/wandb/run-20250216_222311-q54yn29f/run-q54yn29f.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250216_222655-tqohcc9e/files/config.yaml b/algorithm/wandb/run-20250216_222655-tqohcc9e/files/config.yaml
deleted file mode 100644
index 166c7eb..0000000
--- a/algorithm/wandb/run-20250216_222655-tqohcc9e/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739719615.950592
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250216_222655-tqohcc9e/run-tqohcc9e.wandb b/algorithm/wandb/run-20250216_222655-tqohcc9e/run-tqohcc9e.wandb
deleted file mode 100644
index 8825ca9..0000000
Binary files a/algorithm/wandb/run-20250216_222655-tqohcc9e/run-tqohcc9e.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250216_222942-bmi8e8yd/files/config.yaml b/algorithm/wandb/run-20250216_222942-bmi8e8yd/files/config.yaml
deleted file mode 100644
index ebb2be1..0000000
--- a/algorithm/wandb/run-20250216_222942-bmi8e8yd/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739719782.39609
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250216_222942-bmi8e8yd/run-bmi8e8yd.wandb b/algorithm/wandb/run-20250216_222942-bmi8e8yd/run-bmi8e8yd.wandb
deleted file mode 100644
index e69de29..0000000
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250216_223032-pashuyys/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index a374268..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Ant-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 512
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/conda-environment.yaml b/algorithm/wandb/run-20250216_223032-pashuyys/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/config.yaml b/algorithm/wandb/run-20250216_223032-pashuyys/files/config.yaml
deleted file mode 100644
index b45c235..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739719832.69605
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Ant-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/diff.patch b/algorithm/wandb/run-20250216_223032-pashuyys/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/events.out.tfevents.1739719838.Tri.231436.0 b/algorithm/wandb/run-20250216_223032-pashuyys/files/events.out.tfevents.1739719838.Tri.231436.0
deleted file mode 120000
index bfbc247..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/events.out.tfevents.1739719838.Tri.231436.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Ant-v4__sac_rrd_iq__1__1739719829/events.out.tfevents.1739719838.Tri.231436.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/requirements.txt b/algorithm/wandb/run-20250216_223032-pashuyys/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/wandb-metadata.json b/algorithm/wandb/run-20250216_223032-pashuyys/files/wandb-metadata.json
deleted file mode 100644
index f7d6a68..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-16T15:30:36.203713",
-    "startedAt": "2025-02-16T15:30:32.688850",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.843074798583984
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/files/wandb-summary.json b/algorithm/wandb/run-20250216_223032-pashuyys/files/wandb-summary.json
deleted file mode 100644
index fdfb7d8..0000000
--- a/algorithm/wandb/run-20250216_223032-pashuyys/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2799100, "_timestamp": 1739824061.5576067, "_runtime": 104228.86155676842, "_step": 30874, "charts/episodic_return": 779.31787109375, "charts/episodic_length": 1000.0, "losses/qf1_values": 77.89967346191406, "losses/qf2_values": 77.90657806396484, "losses/qf1_loss": 0.0007540980004705489, "losses/qf2_loss": 0.0006867455085739493, "losses/qf_loss": 0.0007204217836260796, "losses/q_loss": 0.035422444343566895, "losses/actor_loss": -77.84021759033203, "losses/alpha": 0.004211192950606346, "losses/re_loss": 5.425868948805146e-06, "rewards/q_r_loss": 0.009135867469012737, "rewards/pre_re": 0.8402594923973083, "rewards/pre_rewards": 0.8469935655593872, "rewards/eps_rewards": 0.8443591594696045, "rewards/batch_rewards": 0.8421065211296082, "rewards/true_r_var": 0.03903420642018318, "rewards/a_var1": 0.03699462115764618, "rewards/a_var2": 0.03390665352344513, "rewards/r_var1": 0.0007540000369772315, "rewards/r_var2": 0.0006432150839827955, "rewards/r_q_v_max": 0.9629592895507812, "rewards/r_q_v_min": 0.5828628540039062, "rewards/r_true_max": 1.015497088432312, "rewards/r_true_min": -1.1853604316711426, "charts/SPS": 26.0, "losses/alpha_loss": 0.003049227176234126}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223032-pashuyys/run-pashuyys.wandb b/algorithm/wandb/run-20250216_223032-pashuyys/run-pashuyys.wandb
deleted file mode 100644
index d815db3..0000000
Binary files a/algorithm/wandb/run-20250216_223032-pashuyys/run-pashuyys.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 24b3b83..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "HalfCheetah-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 512
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/conda-environment.yaml b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/config.yaml b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/config.yaml
deleted file mode 100644
index 2f1aa70..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739719903.260451
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/diff.patch b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/events.out.tfevents.1739719925.Tri.232119.0 b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/events.out.tfevents.1739719925.Tri.232119.0
deleted file mode 120000
index c44def0..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/events.out.tfevents.1739719925.Tri.232119.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/HalfCheetah-v4__sac_rrd_iq__1__1739719894/events.out.tfevents.1739719925.Tri.232119.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/requirements.txt b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/wandb-metadata.json b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/wandb-metadata.json
deleted file mode 100644
index a433300..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-16T15:32:02.158708",
-    "startedAt": "2025-02-16T15:31:43.251211",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.84344482421875
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/wandb-summary.json b/algorithm/wandb/run-20250216_223143-jmeuwoet/files/wandb-summary.json
deleted file mode 100644
index e57501e..0000000
--- a/algorithm/wandb/run-20250216_223143-jmeuwoet/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2836300, "_timestamp": 1739824058.2488687, "_runtime": 104154.98841762543, "_step": 31142, "charts/episodic_return": 9823.1826171875, "charts/episodic_length": 1000.0, "losses/qf1_values": 898.2010498046875, "losses/qf2_values": 898.1593017578125, "losses/qf1_loss": 19.70492935180664, "losses/qf2_loss": 23.16666030883789, "losses/qf_loss": 21.435794830322266, "losses/q_loss": 28.735511779785156, "losses/actor_loss": -897.2545166015625, "losses/alpha": 0.04977423697710037, "losses/re_loss": 0.014778711833059788, "rewards/q_r_loss": 30.34739112854004, "rewards/pre_re": 8.449577331542969, "rewards/pre_rewards": 7.564496040344238, "rewards/eps_rewards": 7.667201995849609, "rewards/batch_rewards": 7.522922992706299, "rewards/true_r_var": 12.201464653015137, "rewards/a_var1": 25.680330276489258, "rewards/a_var2": 27.058849334716797, "rewards/r_var1": 17.65642738342285, "rewards/r_var2": 21.235902786254883, "rewards/r_q_v_max": 46.2508544921875, "rewards/r_q_v_min": -26.93023681640625, "rewards/r_true_max": 12.130074501037598, "rewards/r_true_min": -1.9042894840240479, "charts/SPS": 27.0, "losses/alpha_loss": -0.020722314715385437}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223143-jmeuwoet/run-jmeuwoet.wandb b/algorithm/wandb/run-20250216_223143-jmeuwoet/run-jmeuwoet.wandb
deleted file mode 100644
index dc983ae..0000000
Binary files a/algorithm/wandb/run-20250216_223143-jmeuwoet/run-jmeuwoet.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index ccdb0a3..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "Swimmer-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 512
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/conda-environment.yaml b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/config.yaml b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/config.yaml
deleted file mode 100644
index 0e67975..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739720330.536088
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: Swimmer-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/diff.patch b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/events.out.tfevents.1739720337.Tri.234878.0 b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/events.out.tfevents.1739720337.Tri.234878.0
deleted file mode 120000
index fc91bc7..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/events.out.tfevents.1739720337.Tri.234878.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/Swimmer-v4__sac_rrd_iq__1__1739720325/events.out.tfevents.1739720337.Tri.234878.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/requirements.txt b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/wandb-metadata.json b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/wandb-metadata.json
deleted file mode 100644
index febdb28..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-16T15:38:51.723302",
-    "startedAt": "2025-02-16T15:38:50.522641",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.845333099365234
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/wandb-summary.json b/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/wandb-summary.json
deleted file mode 100644
index 4eb34fe..0000000
--- a/algorithm/wandb/run-20250216_223850-pg2vvhkx/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2824100, "_timestamp": 1739824061.3241618, "_runtime": 103730.78807377815, "_step": 31011, "charts/episodic_return": 34.550533294677734, "charts/episodic_length": 1000.0, "losses/qf1_values": 10.308921813964844, "losses/qf2_values": 10.322159767150879, "losses/qf1_loss": 0.0006331795011647046, "losses/qf2_loss": 0.00047725773765705526, "losses/qf_loss": 0.0005552186048589647, "losses/q_loss": 0.6021614670753479, "losses/actor_loss": -10.320114135742188, "losses/alpha": 0.0010329498909413815, "losses/re_loss": 5.827338554809103e-06, "rewards/q_r_loss": 0.0021852701902389526, "rewards/pre_re": 0.049396589398384094, "rewards/pre_rewards": 0.06553860008716583, "rewards/eps_rewards": 0.0678318664431572, "rewards/batch_rewards": 0.0586557537317276, "rewards/true_r_var": 0.6053198575973511, "rewards/a_var1": 0.6053802967071533, "rewards/a_var2": 0.6012099385261536, "rewards/r_var1": 0.0004112645401619375, "rewards/r_var2": 0.0004745055630337447, "rewards/r_q_v_max": 0.11443614959716797, "rewards/r_q_v_min": -0.022161483764648438, "rewards/r_true_max": 1.5509775876998901, "rewards/r_true_min": -1.044265627861023, "charts/SPS": 27.0, "losses/alpha_loss": 0.0004980648518539965}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_223850-pg2vvhkx/run-pg2vvhkx.wandb b/algorithm/wandb/run-20250216_223850-pg2vvhkx/run-pg2vvhkx.wandb
deleted file mode 100644
index 7e6b98b..0000000
Binary files a/algorithm/wandb/run-20250216_223850-pg2vvhkx/run-pg2vvhkx.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 24b3b83..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,465 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "HalfCheetah-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 512
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/conda-environment.yaml b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/config.yaml b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/config.yaml
deleted file mode 100644
index 79771eb..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/config.yaml
+++ /dev/null
@@ -1,100 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1739721303.187009
-    t:
-      1:
-      - 1
-      - 55
-      2:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/diff.patch b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/events.out.tfevents.1739721309.Tri.240062.0 b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/events.out.tfevents.1739721309.Tri.240062.0
deleted file mode 120000
index 251a600..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/events.out.tfevents.1739721309.Tri.240062.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/HalfCheetah-v4__sac_rrd_iq__1__1739721299/events.out.tfevents.1739721309.Tri.240062.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/requirements.txt b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/wandb-metadata.json b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/wandb-metadata.json
deleted file mode 100644
index 103f629..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-16T15:55:04.471420",
-    "startedAt": "2025-02-16T15:55:03.178596",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2687.998,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2687.998,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 17.84994888305664
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/wandb-summary.json b/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/wandb-summary.json
deleted file mode 100644
index d66a607..0000000
--- a/algorithm/wandb/run-20250216_225503-kyz3lvc7/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{"global_step": 2773000, "_timestamp": 1739824061.3196046, "_runtime": 102758.1325955391, "_step": 30451, "charts/episodic_return": 9780.33203125, "charts/episodic_length": 1000.0, "losses/qf1_values": 908.382080078125, "losses/qf2_values": 908.361572265625, "losses/qf1_loss": 1.3964641094207764, "losses/qf2_loss": 1.0018872022628784, "losses/qf_loss": 1.1991755962371826, "losses/q_loss": 3.615133047103882, "losses/actor_loss": -908.2476806640625, "losses/alpha": 0.04701758548617363, "losses/re_loss": 0.0063987672328948975, "rewards/q_r_loss": 2.308070182800293, "rewards/pre_re": 9.069375038146973, "rewards/pre_rewards": 9.705613136291504, "rewards/eps_rewards": 9.63618278503418, "rewards/batch_rewards": 9.953707695007324, "rewards/true_r_var": 2.477203845977783, "rewards/a_var1": 3.413105010986328, "rewards/a_var2": 3.118333578109741, "rewards/r_var1": 1.324308156967163, "rewards/r_var2": 0.9182953238487244, "rewards/r_q_v_max": 12.93701171875, "rewards/r_q_v_min": 5.07452392578125, "rewards/r_true_max": 12.520294189453125, "rewards/r_true_min": -0.9031215310096741, "charts/SPS": 26.0, "losses/alpha_loss": 0.006273978389799595}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250216_225503-kyz3lvc7/run-kyz3lvc7.wandb b/algorithm/wandb/run-20250216_225503-kyz3lvc7/run-kyz3lvc7.wandb
deleted file mode 100644
index 3e8b947..0000000
Binary files a/algorithm/wandb/run-20250216_225503-kyz3lvc7/run-kyz3lvc7.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250223_192522-zy6f4uuy/files/config.yaml b/algorithm/wandb/run-20250223_192522-zy6f4uuy/files/config.yaml
deleted file mode 100644
index ef59079..0000000
--- a/algorithm/wandb/run-20250223_192522-zy6f4uuy/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1740313522.394054
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250223_192522-zy6f4uuy/run-zy6f4uuy.wandb b/algorithm/wandb/run-20250223_192522-zy6f4uuy/run-zy6f4uuy.wandb
deleted file mode 100644
index a4f3170..0000000
Binary files a/algorithm/wandb/run-20250223_192522-zy6f4uuy/run-zy6f4uuy.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250223_192850-jmntexev/files/config.yaml b/algorithm/wandb/run-20250223_192850-jmntexev/files/config.yaml
deleted file mode 100644
index bfbae1a..0000000
--- a/algorithm/wandb/run-20250223_192850-jmntexev/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1740313730.807855
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250223_192850-jmntexev/run-jmntexev.wandb b/algorithm/wandb/run-20250223_192850-jmntexev/run-jmntexev.wandb
deleted file mode 100644
index 95b8fdb..0000000
Binary files a/algorithm/wandb/run-20250223_192850-jmntexev/run-jmntexev.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250223_193221-hcgp9v3p/files/config.yaml b/algorithm/wandb/run-20250223_193221-hcgp9v3p/files/config.yaml
deleted file mode 100644
index 5bfbc16..0000000
--- a/algorithm/wandb/run-20250223_193221-hcgp9v3p/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1740313941.256108
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 5
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250223_193221-hcgp9v3p/run-hcgp9v3p.wandb b/algorithm/wandb/run-20250223_193221-hcgp9v3p/run-hcgp9v3p.wandb
deleted file mode 100644
index 0358494..0000000
Binary files a/algorithm/wandb/run-20250223_193221-hcgp9v3p/run-hcgp9v3p.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/code/algorithm/sac_rrd_iq.py b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/code/algorithm/sac_rrd_iq.py
deleted file mode 100644
index 4d8f1f9..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/code/algorithm/sac_rrd_iq.py
+++ /dev/null
@@ -1,467 +0,0 @@
-# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/sac/#sac_continuous_actionpy
-import os
-import random
-import time
-from dataclasses import dataclass
-
-import gymnasium as gym
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-import tyro
-from stable_baselines3.common.buffers import ReplayBuffer
-from buffers import TrajectoryReplayBuffer
-from torch.utils.tensorboard import SummaryWriter
-
-
-@dataclass
-class Args:
-    exp_name: str = os.path.basename(__file__)[: -len(".py")]
-    """the name of this experiment"""
-    seed: int = 1
-    """seed of the experiment"""
-    torch_deterministic: bool = True
-    """if toggled, `torch.backends.cudnn.deterministic=False`"""
-    cuda: bool = True
-    """if toggled, cuda will be enabled by default"""
-    track: bool = True
-    """if toggled, this experiment will be tracked with Weights and Biases"""
-    wandb_project_name: str = "cleanRL"
-    """the wandb's project name"""
-    wandb_entity: str = None
-    """the entity (team) of wandb's project"""
-    capture_video: bool = False
-    """whether to capture videos of the agent performances (check out `videos` folder)"""
-
-    # Algorithm specific arguments
-    env_id: str = "HalfCheetah-v4"
-    """the environment id of the task"""
-    total_timesteps: int = 3000000
-    """total timesteps of the experiments"""
-    buffer_size: int = int(2e3)
-    """the replay memory buffer size"""
-    trajectory_size: int = int(1e3)
-    """the replay memory buffer size"""
-    gamma: float = 0.99
-    """the discount factor gamma"""
-    tau: float = 0.005
-    """target smoothing coefficient (default: 0.005)"""
-    batch_size: int = 512
-    """the batch size of sample from the reply memory"""
-    learning_starts: int = 5e3
-    """timestep to start learning"""
-    policy_lr: float = 3e-4
-    """the learning rate of the policy network optimizer"""
-    q_lr: float = 1e-3
-    """the learning rate of the Q network network optimizer"""
-    policy_frequency: int = 2
-    """the frequency of training policy (delayed)"""
-    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
-    """the frequency of updates for the target nerworks"""
-    alpha: float = 0.2
-    """Entropy regularization coefficient."""
-    autotune: bool = True
-    """automatic tuning of the entropy coefficient"""
-    q_coefficient: float = 0
-
-    r_coefficient: float = 1
-def make_env(env_id, seed, idx, capture_video, run_name):
-    def thunk():
-        if capture_video and idx == 0:
-            env = gym.make(env_id, render_mode="rgb_array")
-            env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-        else:
-            env = gym.make(env_id)
-        env = gym.wrappers.RecordEpisodeStatistics(env)
-        env.action_space.seed(seed)
-        return env
-
-    return thunk
-
-
-# ALGO LOGIC: initialize agent here:
-class SoftQNetwork(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc3 = nn.Linear(256, 1)
-
-    def forward(self, x, a):
-        x = torch.cat([x, a], 1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        x = self.fc3(x)
-        return x
-
-
-LOG_STD_MAX = 2
-LOG_STD_MIN = -5
-# Sua buffer return trajectory, 
-class Reward(nn.Module):
-    def __init__(self, env):
-        super().__init__()  
-        obs_dim=np.array(env.single_observation_space.shape).prod()
-        act_dim=np.prod(env.single_action_space.shape)
-        
-        self.fc1 = nn.Linear(2*obs_dim + act_dim, 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_q = nn.Linear(256, 1)
-
-    def forward(self, obs_ph, acts, next_obs_ph):
-        x = torch.cat([obs_ph, acts, next_obs_ph], dim=-1)
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        q_value =  self.fc_q(x)
-        return q_value
-
-class Actor(nn.Module):
-    def __init__(self, env):
-        super().__init__()
-        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)
-        self.fc2 = nn.Linear(256, 256)
-        self.fc_mean = nn.Linear(256, np.prod(env.single_action_space.shape))
-        self.fc_logstd = nn.Linear(256, np.prod(env.single_action_space.shape))
-        # action rescaling
-        self.register_buffer(
-            "action_scale", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-        self.register_buffer(
-            "action_bias", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)
-        )
-
-    def forward(self, x):
-        x = F.relu(self.fc1(x))
-        x = F.relu(self.fc2(x))
-        mean = self.fc_mean(x)
-        log_std = self.fc_logstd(x)
-        log_std = torch.tanh(log_std)
-        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats
-
-        return mean, log_std
-
-    def get_action(self, x):
-        mean, log_std = self(x)
-        std = log_std.exp()
-        normal = torch.distributions.Normal(mean, std)
-        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
-        y_t = torch.tanh(x_t)
-        action = y_t * self.action_scale + self.action_bias
-        log_prob = normal.log_prob(x_t)
-        # Enforcing Action Bound
-        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
-        log_prob = log_prob.sum(1, keepdim=True)
-        mean = torch.tanh(mean) * self.action_scale + self.action_bias
-        return action, log_prob, mean
-
-
-if __name__ == "__main__":
-    import stable_baselines3 as sb3
-
-    if sb3.__version__ < "2.0":
-        raise ValueError(
-            """Ongoing migration: run the following command to install the new dependencies:
-poetry run pip install "stable_baselines3==2.0.0a1"
-"""
-        )
-
-    args = tyro.cli(Args)
-    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-    if args.track:
-        import wandb
-
-        wandb.init(
-            settings=wandb.Settings(start_method="fork"),
-            project=args.wandb_project_name,
-            entity=args.wandb_entity,
-            sync_tensorboard=True,
-            config=vars(args),
-            name=run_name,
-            monitor_gym=True,
-            
-            save_code=True,
-        )
-    writer = SummaryWriter(f"runs/{run_name}")
-    writer.add_text(
-        "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-    )
-
-    # TRY NOT TO MODIFY: seeding
-    random.seed(args.seed)
-    np.random.seed(args.seed)
-    torch.manual_seed(args.seed)
-    torch.backends.cudnn.deterministic = args.torch_deterministic
-
-    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
-
-    # env setup
-    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])
-    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
-
-    max_action = float(envs.single_action_space.high[0])
-
-    actor = Actor(envs).to(device)
-    reward_net = Reward(envs).to(device)
-    qf1 = SoftQNetwork(envs).to(device)
-    qf2 = SoftQNetwork(envs).to(device)
-    qf1_target = SoftQNetwork(envs).to(device)
-    qf2_target = SoftQNetwork(envs).to(device)
-    qf1_target.load_state_dict(qf1.state_dict())
-    qf2_target.load_state_dict(qf2.state_dict())
-    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
-    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
-    re_optimizer = optim.Adam(list(reward_net.parameters()), lr=args.policy_lr)
-
-    # Automatic entropy tuning
-    if args.autotune:
-        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
-        log_alpha = torch.zeros(1, requires_grad=True, device=device)
-        alpha = log_alpha.exp().item()
-        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
-    else:
-        alpha = args.alpha
-
-    envs.single_observation_space.dtype = np.float32
-    rb = TrajectoryReplayBuffer(
-        args.trajectory_size,
-        args.buffer_size,
-        envs.single_observation_space,
-        envs.single_action_space,
-        device,
-        handle_timeout_termination=False,
-    )
-    start_time = time.time()
-
-    # TRY NOT TO MODIFY: start the game
-    obs, _ = envs.reset(seed=args.seed)
-    for global_step in range(args.total_timesteps):
-        # ALGO LOGIC: put action logic here
-        if global_step < args.learning_starts:
-            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
-        else:
-            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))
-            actions = actions.detach().cpu().numpy()
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, rewards, terminations, truncations, infos = envs.step(actions)
-
-        # TRY NOT TO MODIFY: record rewards for plotting purposes
-        if "final_info" in infos:
-            # print(infos["final_info"])
-            # exit()
-            for info in infos["final_info"]:
-                print(f"global_step={global_step}, episodic_return={info['episode']['r']},  episodic_length={info['episode']['l']} ")
-
-                writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["episode"]["l"], global_step)
-                break
-
-        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`
-        real_next_obs = next_obs.copy()
-        for idx, trunc in enumerate(truncations):
-            if trunc:
-                real_next_obs[idx] = infos["final_observation"][idx]
-        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
-        # print(terminations.shape)
-        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
-        obs = next_obs
-        # ALGO LOGIC: training.
-        if global_step > args.learning_starts:
-            data = rb.sample(args.batch_size)
-          
-            mb_rewards = data.rewards
-            mb_obs = data.observations
-            mb_act = data.actions
-            mb_obs_next = data.next_observations
-            mb_dones = data.dones
-            mb_eps_rewards=data.eps_rewards
-            # print(mb_rewards.shape)
-            # print(mb_obs.shape)
-            # print(mb_act.shape)
-            # print(mb_obs_next.shape)
-            # print(mb_dones.shape)
-            # print(mb_eps_rewards)
-            # print(mb_eps_rewards.shape)
-            # exit()
-
-
-            pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-            # print(pre_rewards.shape)
-
-            re_loss = F.mse_loss(torch.mean(pre_rewards), torch.mean(mb_eps_rewards))
-            # print(loss_re)
-
-            re_optimizer.zero_grad()
-            re_loss.backward()
-            re_optimizer.step()
-
-
-
-            
-            with torch.no_grad():
-
-                pre_rewards = reward_net(mb_obs, mb_act, mb_obs_next)
-
-                next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)
-                qf1_next_target = qf1_target(data.next_observations, next_state_actions)
-                qf2_next_target = qf2_target(data.next_observations, next_state_actions)
-
-                # qf1_next_target = qf1(data.next_observations, next_state_actions)
-                # qf2_next_target = qf2(data.next_observations, next_state_actions)
-
-                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
-                next_v_value = (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                # next_q_value = pre_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-                next_q_value = mb_rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
-
-
-       
-            qf1_a_values = qf1(data.observations, data.actions).view(-1)
-            qf2_a_values = qf2(data.observations, data.actions).view(-1)
-
-            
-            # pre_re_1 = qf1_a_values - next_v_value
-            # pre_re_2 = qf2_a_values - next_v_value
-            # next_v_value1 = (1 - data.dones.flatten()) * args.gamma * (qf1_next_target).view(-1)
-            # next_v_value2 = (1 - data.dones.flatten()) * args.gamma * (qf2_next_target).view(-1)
-
-
-
-
-
-            pre_re_1 = qf1_a_values - next_v_value
-            pre_re_2 = qf2_a_values - next_v_value
-
-            # a1= qf1_a_values - qf2_a_values.detach()
-            # a2= qf2_a_values - qf1_a_values.detach()
-
-            a1= pre_re_1 - mb_rewards.flatten()
-            a2= pre_re_2 - mb_rewards.flatten()
-
-
-            n = args.batch_size
-
-            r_mean1 = pre_re_1.mean()
-            r_var_single1 = ((pre_re_1 - r_mean1) ** 2).sum() / (n - 1)
-            r_var1 = (r_var_single1 ).mean()
-
-            r_mean2 = pre_re_2.mean()
-            r_var_single2 = ((pre_re_2 - r_mean2) ** 2).sum() / (n - 1)
-            r_var2 = (r_var_single2 ).mean()
-
-            a_mean1 = a1.mean()
-            a_var_single1 = ((a1 - a_mean1) ** 2).sum() / (n - 1)
-            a_var1 = (a_var_single1 ).mean()
-
-            a_mean2 = a2.mean()
-            a_var_single2 = ((a2 - a_mean2) ** 2).sum() / (n - 1)
-            a_var2 = (a_var_single2 ).mean()
-
-
-            r_mean = mb_rewards.mean()
-            r_var_single = ((mb_rewards - r_mean) ** 2).sum() / (n - 1)
-            r_var = (r_var_single ).mean()
- 
-
-            # qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) - r_var1 - a_var1
-            # qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) - r_var2 - a_var2
-
-            qf1_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_1)) + args.r_coefficient * r_var1 + args.q_coefficient * torch.mean(pre_re_1**2)
-            qf2_loss = F.mse_loss(torch.mean(mb_eps_rewards), torch.mean(pre_re_2)) + args.r_coefficient * r_var2 + args.q_coefficient * torch.mean(pre_re_2**2)
-
-
-            
-            q1_loss = F.mse_loss(pre_re_1, mb_rewards.flatten())
-            q2_loss = F.mse_loss(pre_re_2, mb_rewards.flatten())
-
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value))
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value))
-
-            
-            # qf1_loss = F.mse_loss(torch.mean(qf1_a_values), torch.mean(next_q_value)) + 2* a_var1 +0.5 * torch.mean(pre_re_1**2)
-            # qf2_loss = F.mse_loss(torch.mean(qf2_a_values), torch.mean(next_q_value)) + 2* a_var2 + 0.5  * torch.mean(pre_re_2**2)
-
-
-            qf_loss = qf1_loss + qf2_loss
-            q_loss = q1_loss + q2_loss
-
-            pre_re = torch.min(pre_re_1, pre_re_2)
-
-            q_r_loss = F.mse_loss(pre_re, pre_rewards.flatten())
-
-            q_optimizer.zero_grad()
-            qf_loss.backward()
-            q_optimizer.step()
-
-
-            if global_step % args.policy_frequency == 0:  # TD 3 Delayed update support
-                for _ in range(
-                    args.policy_frequency
-                ):  # compensate for the delay by doing 'actor_update_interval' instead of 1
-                    pi, log_pi, _ = actor.get_action(data.observations)
-                    qf1_pi = qf1(data.observations, pi)
-                    qf2_pi = qf2(data.observations, pi)
-                    min_qf_pi = torch.min(qf1_pi, qf2_pi)
-                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
-
-                    actor_optimizer.zero_grad()
-                    actor_loss.backward()
-                    actor_optimizer.step()
-
-                    if args.autotune:
-                        with torch.no_grad():
-                            _, log_pi, _ = actor.get_action(data.observations)
-                        alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
-
-                        a_optimizer.zero_grad()
-                        alpha_loss.backward()
-                        a_optimizer.step()
-                        alpha = log_alpha.exp().item()
-
-            # update the target networks
-            if global_step % args.target_network_frequency == 0:
-                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
-                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
-
-            if global_step % 100 == 0:
-                writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
-                writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
-                writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
-                writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/q_loss", q_loss.item() / 2.0, global_step)
-                writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
-                writer.add_scalar("losses/alpha", alpha, global_step)
-                writer.add_scalar("losses/re_loss", re_loss.item(), global_step)
-                writer.add_scalar("rewards/q_r_loss", q_r_loss.item(), global_step)
-                writer.add_scalar("rewards/pre_re", pre_re.mean().item(), global_step)
-                writer.add_scalar("rewards/pre_rewards", pre_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/eps_rewards", mb_eps_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/batch_rewards", mb_rewards.mean().item(), global_step)
-                writer.add_scalar("rewards/true_r_var", r_var.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var1", a_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/a_var2", a_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var1", r_var1.mean().item(), global_step)
-                writer.add_scalar("rewards/r_var2", r_var2.mean().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_max", pre_re.max().item(), global_step)
-                writer.add_scalar("rewards/r_q_v_min", pre_re.min().item(), global_step)
-                writer.add_scalar("rewards/r_true_max", mb_rewards.flatten().max().item(), global_step)
-                writer.add_scalar("rewards/r_true_min", mb_rewards.flatten().min().item(), global_step)
-
-
-
-
-
-                # print("SPS:", int(global_step / (time.time() - start_time)))
-                writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-                if args.autotune:
-                    writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
-
-    envs.close()
-    writer.close()
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/conda-environment.yaml b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/conda-environment.yaml
deleted file mode 100644
index ada4ad6..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/conda-environment.yaml
+++ /dev/null
@@ -1,123 +0,0 @@
-name: mujoco
-channels:
-  - defaults
-  - https://repo.anaconda.com/pkgs/main
-  - https://repo.anaconda.com/pkgs/r
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - ca-certificates=2024.12.31=h06a4308_0
-  - ld_impl_linux-64=2.40=h12ee557_0
-  - libffi=3.4.4=h6a678d5_1
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - ncurses=6.4=h6a678d5_0
-  - openssl=3.0.15=h5eee18b_0
-  - pip=24.2=py38h06a4308_0
-  - python=3.8.20=he870216_0
-  - readline=8.2=h5eee18b_0
-  - setuptools=75.1.0=py38h06a4308_0
-  - sqlite=3.45.3=h5eee18b_0
-  - tk=8.6.14=h39e8969_0
-  - wheel=0.44.0=py38h06a4308_0
-  - xz=5.4.6=h5eee18b_1
-  - zlib=1.2.13=h5eee18b_1
-  - pip:
-      - absl-py==2.1.0
-      - appdirs==1.4.4
-      - cachetools==5.5.0
-      - certifi==2024.12.14
-      - cffi==1.17.1
-      - charset-normalizer==3.4.1
-      - click==8.1.8
-      - cloudpickle==3.1.0
-      - contourpy==1.1.1
-      - cycler==0.12.1
-      - cython==3.0.11
-      - docker-pycreds==0.4.0
-      - docstring-parser==0.16
-      - etils==1.3.0
-      - eval-type-backport==0.2.2
-      - farama-notifications==0.0.4
-      - fasteners==0.19
-      - filelock==3.16.1
-      - fonttools==4.55.3
-      - fsspec==2024.12.0
-      - gitdb==4.0.12
-      - gitpython==3.1.44
-      - glfw==2.8.0
-      - google-auth==2.37.0
-      - google-auth-oauthlib==1.0.0
-      - grpcio==1.69.0
-      - gym==0.23.1
-      - gym-notices==0.0.8
-      - gymnasium==0.28.1
-      - idna==3.10
-      - imageio==2.35.1
-      - importlib-metadata==8.5.0
-      - importlib-resources==6.4.5
-      - jax-jumpy==1.0.0
-      - jinja2==3.1.5
-      - kiwisolver==1.4.7
-      - markdown==3.7
-      - markdown-it-py==3.0.0
-      - markupsafe==2.1.5
-      - matplotlib==3.7.5
-      - mdurl==0.1.2
-      - mpmath==1.3.0
-      - mujoco==2.3.3
-      - networkx==3.1
-      - numpy==1.24.4
-      - nvidia-cublas-cu12==12.1.3.1
-      - nvidia-cuda-cupti-cu12==12.1.105
-      - nvidia-cuda-nvrtc-cu12==12.1.105
-      - nvidia-cuda-runtime-cu12==12.1.105
-      - nvidia-cudnn-cu12==9.1.0.70
-      - nvidia-cufft-cu12==11.0.2.54
-      - nvidia-curand-cu12==10.3.2.106
-      - nvidia-cusolver-cu12==11.4.5.107
-      - nvidia-cusparse-cu12==12.1.0.106
-      - nvidia-nccl-cu12==2.20.5
-      - nvidia-nvjitlink-cu12==12.6.85
-      - nvidia-nvtx-cu12==12.1.105
-      - oauthlib==3.2.2
-      - packaging==24.2
-      - pandas==2.0.3
-      - pathtools==0.1.2
-      - pillow==10.4.0
-      - protobuf==4.25.6
-      - psutil==6.1.1
-      - pyasn1==0.6.1
-      - pyasn1-modules==0.4.1
-      - pycparser==2.22
-      - pygments==2.19.1
-      - pyopengl==3.1.7
-      - pyparsing==3.1.4
-      - python-dateutil==2.9.0.post0
-      - pytz==2024.2
-      - pyyaml==6.0.2
-      - requests==2.32.3
-      - requests-oauthlib==2.0.0
-      - rich==13.9.4
-      - rsa==4.9
-      - sentry-sdk==2.20.0
-      - setproctitle==1.3.4
-      - shtab==1.7.1
-      - six==1.17.0
-      - smmap==5.0.2
-      - stable-baselines3==2.4.1
-      - sympy==1.13.3
-      - tensorboard==2.14.0
-      - tensorboard-data-server==0.7.2
-      - torch==2.4.1
-      - triton==3.0.0
-      - typeguard==4.4.0
-      - typing-extensions==4.12.2
-      - tyro==0.9.6
-      - tzdata==2024.2
-      - urllib3==2.2.3
-      - wandb==0.13.11
-      - werkzeug==3.0.6
-      - zipp==3.20.2
-prefix: /home/tri/miniconda3/envs/mujoco
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/config.yaml b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/config.yaml
deleted file mode 100644
index 4d6a9e3..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1740314138.503842
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 6
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 0
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/diff.patch b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/diff.patch
deleted file mode 100644
index 247b53d..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/diff.patch
+++ /dev/null
@@ -1,47 +0,0 @@
-diff --git a/.vscode/launch.json b/.vscode/launch.json
-index 8490e0a..6ec5265 100644
---- a/.vscode/launch.json
-+++ b/.vscode/launch.json
-@@ -1,24 +1,15 @@
- {
-     "version": "0.2.0",
-     "configurations": [
-+        
-+
-         {
--            "name": "Python Debugger: train.py with Arguments",
-+            "name": "Python Debugger: Python files",
-             "type": "debugpy",
-             "request": "launch",
--            "program": "${workspaceFolder}/train.py",
-+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
-             "console": "integratedTerminal",
--            "args": [
--                "--tag","Atari_256_32",
--                "--alg", "rrd_atari_pytorch",
--                "--basis_alg", "dqn",
--                "--code","pytorch",
--                "--rrd_bias_correction", "True",
--                "--env", "Assault",
--                "--rrd_batch_size","1024",
--                "--rrd_sample_size","32",
--
--
--            ]
-+            
-         }
-     ]
- }
-diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
-index d0e9181..75321c3 100644
---- a/algorithm/rrd_mujoco_pytorch.py
-+++ b/algorithm/rrd_mujoco_pytorch.py
-@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
-             # Calculate Q-value for next state-action pair
-             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
-  
--            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
-+            # Reward calculation: r = Q(s, a) - γ * V(s')
-             reward = q_value - self.args.gamma * q_value_next
-  
-             # Reshape if input was flattened
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/events.out.tfevents.1740314204.Tri.280814.0 b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/events.out.tfevents.1740314204.Tri.280814.0
deleted file mode 120000
index 8d25d84..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/events.out.tfevents.1740314204.Tri.280814.0
+++ /dev/null
@@ -1 +0,0 @@
-/home/tri/offline_rl/No_reward/algorithm/runs/HalfCheetah-v4__sac_rrd_iq__1__1740314131/events.out.tfevents.1740314204.Tri.280814.0
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/requirements.txt b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/requirements.txt
deleted file mode 100644
index c3e25d3..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/requirements.txt
+++ /dev/null
@@ -1,109 +0,0 @@
-absl-py==2.1.0
-appdirs==1.4.4
-autocommand==2.2.2
-backports.tarfile==1.2.0
-cachetools==5.5.0
-certifi==2024.12.14
-cffi==1.17.1
-charset-normalizer==3.4.1
-click==8.1.8
-cloudpickle==3.1.0
-contourpy==1.1.1
-cycler==0.12.1
-cython==3.0.11
-docker-pycreds==0.4.0
-docstring-parser==0.16
-etils==1.3.0
-eval-type-backport==0.2.2
-farama-notifications==0.0.4
-fasteners==0.19
-filelock==3.16.1
-fonttools==4.55.3
-fsspec==2024.12.0
-gitdb==4.0.12
-gitpython==3.1.44
-glfw==2.8.0
-google-auth-oauthlib==1.0.0
-google-auth==2.37.0
-grpcio==1.69.0
-gym-notices==0.0.8
-gym==0.23.1
-gymnasium==0.28.1
-idna==3.10
-imageio==2.35.1
-importlib-metadata==8.5.0
-importlib-resources==6.4.5
-inflect==7.3.1
-jaraco.collections==5.1.0
-jaraco.context==5.3.0
-jaraco.functools==4.0.1
-jaraco.text==3.12.1
-jax-jumpy==1.0.0
-jinja2==3.1.5
-kiwisolver==1.4.7
-markdown-it-py==3.0.0
-markdown==3.7
-markupsafe==2.1.5
-matplotlib==3.7.5
-mdurl==0.1.2
-more-itertools==10.3.0
-mpmath==1.3.0
-mujoco==2.3.3
-networkx==3.1
-numpy==1.24.4
-nvidia-cublas-cu12==12.1.3.1
-nvidia-cuda-cupti-cu12==12.1.105
-nvidia-cuda-nvrtc-cu12==12.1.105
-nvidia-cuda-runtime-cu12==12.1.105
-nvidia-cudnn-cu12==9.1.0.70
-nvidia-cufft-cu12==11.0.2.54
-nvidia-curand-cu12==10.3.2.106
-nvidia-cusolver-cu12==11.4.5.107
-nvidia-cusparse-cu12==12.1.0.106
-nvidia-nccl-cu12==2.20.5
-nvidia-nvjitlink-cu12==12.6.85
-nvidia-nvtx-cu12==12.1.105
-oauthlib==3.2.2
-packaging==24.2
-pandas==2.0.3
-pathtools==0.1.2
-pillow==10.4.0
-pip==24.2
-platformdirs==4.2.2
-protobuf==4.25.6
-psutil==6.1.1
-pyasn1-modules==0.4.1
-pyasn1==0.6.1
-pycparser==2.22
-pygments==2.19.1
-pyopengl==3.1.7
-pyparsing==3.1.4
-python-dateutil==2.9.0.post0
-pytz==2024.2
-pyyaml==6.0.2
-requests-oauthlib==2.0.0
-requests==2.32.3
-rich==13.9.4
-rsa==4.9
-sentry-sdk==2.20.0
-setproctitle==1.3.4
-setuptools==75.1.0
-shtab==1.7.1
-six==1.17.0
-smmap==5.0.2
-stable-baselines3==2.4.1
-sympy==1.13.3
-tensorboard-data-server==0.7.2
-tensorboard==2.14.0
-tomli==2.0.1
-torch==2.4.1
-triton==3.0.0
-typeguard==4.4.0
-typing-extensions==4.12.2
-tyro==0.9.6
-tzdata==2024.2
-urllib3==2.2.3
-wandb==0.13.11
-werkzeug==3.0.6
-wheel==0.44.0
-zipp==3.20.2
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/wandb-metadata.json b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/wandb-metadata.json
deleted file mode 100644
index 1b2f430..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/wandb-metadata.json
+++ /dev/null
@@ -1,125 +0,0 @@
-{
-    "os": "Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.17",
-    "python": "3.8.20",
-    "heartbeatAt": "2025-02-23T12:36:40.360743",
-    "startedAt": "2025-02-23T12:35:38.491081",
-    "docker": null,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "sac_rrd_iq.py",
-    "codePath": "algorithm/sac_rrd_iq.py",
-    "git": {
-        "remote": "https://github.com/trito11/No_reward.git",
-        "commit": "4442c921304742160ebe1c9dae09c8696902da94"
-    },
-    "email": null,
-    "root": "/home/tri/offline_rl/No_reward",
-    "host": "Tri",
-    "username": "tri",
-    "executable": "/home/tri/miniconda3/envs/mujoco/bin/python",
-    "cpu_count": 8,
-    "cpu_count_logical": 16,
-    "cpu_freq": {
-        "current": 2688.002,
-        "min": 0.0,
-        "max": 0.0
-    },
-    "cpu_freq_per_core": [
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        },
-        {
-            "current": 2688.002,
-            "min": 0.0,
-            "max": 0.0
-        }
-    ],
-    "disk": {
-        "total": 1006.853931427002,
-        "used": 18.301841735839844
-    },
-    "gpu": "NVIDIA GeForce RTX 4050 Laptop GPU",
-    "gpu_count": 1,
-    "gpu_devices": [
-        {
-            "name": "NVIDIA GeForce RTX 4050 Laptop GPU",
-            "memory_total": 6439305216
-        }
-    ],
-    "memory": {
-        "total": 7.6136016845703125
-    }
-}
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/wandb-summary.json b/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/algorithm/wandb/run-20250223_193538-pdgtp6m7/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/algorithm/wandb/run-20250223_193538-pdgtp6m7/run-pdgtp6m7.wandb b/algorithm/wandb/run-20250223_193538-pdgtp6m7/run-pdgtp6m7.wandb
deleted file mode 100644
index 65bd59d..0000000
Binary files a/algorithm/wandb/run-20250223_193538-pdgtp6m7/run-pdgtp6m7.wandb and /dev/null differ
diff --git a/algorithm/wandb/run-20250223_194552-qtgzrth9/files/config.yaml b/algorithm/wandb/run-20250223_194552-qtgzrth9/files/config.yaml
deleted file mode 100644
index 3f050d1..0000000
--- a/algorithm/wandb/run-20250223_194552-qtgzrth9/files/config.yaml
+++ /dev/null
@@ -1,97 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.11
-    code_path: code/algorithm/sac_rrd_iq.py
-    framework: torch
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.8.20
-    start_time: 1740314752.994873
-    t:
-      1:
-      - 1
-      - 55
-      3:
-      - 13
-      - 16
-      - 23
-      - 35
-      4: 3.8.20
-      5: 0.13.11
-      8:
-      - 6
-alpha:
-  desc: null
-  value: 0.2
-autotune:
-  desc: null
-  value: true
-batch_size:
-  desc: null
-  value: 512
-buffer_size:
-  desc: null
-  value: 2000
-capture_video:
-  desc: null
-  value: false
-cuda:
-  desc: null
-  value: true
-env_id:
-  desc: null
-  value: HalfCheetah-v4
-exp_name:
-  desc: null
-  value: sac_rrd_iq
-gamma:
-  desc: null
-  value: 0.99
-learning_starts:
-  desc: null
-  value: 5000.0
-policy_frequency:
-  desc: null
-  value: 2
-policy_lr:
-  desc: null
-  value: 0.0003
-q_coefficient:
-  desc: null
-  value: 1
-q_lr:
-  desc: null
-  value: 0.001
-r_coefficient:
-  desc: null
-  value: 0.1
-seed:
-  desc: null
-  value: 1
-target_network_frequency:
-  desc: null
-  value: 1
-tau:
-  desc: null
-  value: 0.005
-torch_deterministic:
-  desc: null
-  value: true
-total_timesteps:
-  desc: null
-  value: 3000000
-track:
-  desc: null
-  value: true
-trajectory_size:
-  desc: null
-  value: 1000
-wandb_entity:
-  desc: null
-  value: null
-wandb_project_name:
-  desc: null
-  value: cleanRL
diff --git a/algorithm/wandb/run-20250223_194552-qtgzrth9/run-qtgzrth9.wandb b/algorithm/wandb/run-20250223_194552-qtgzrth9/run-qtgzrth9.wandb
deleted file mode 100644
index d798548..0000000
Binary files a/algorithm/wandb/run-20250223_194552-qtgzrth9/run-qtgzrth9.wandb and /dev/null differ
diff --git a/requirements.txt b/requirements.txt
index 446cebe..795724f 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,17 +1,96 @@
-atari-py==0.2.9
-beautifultable==0.8.0
-Brotli==1.0.9
-cmake==3.25.0
-envpool==0.8.4
-lit==15.0.7
+absl-py==2.1.0
+appdirs==1.4.4
+cachetools==5.5.0
+certifi==2024.12.14
+cffi==1.17.1
+charset-normalizer==3.4.1
+click==8.1.8
+cloudpickle==3.1.0
+contourpy==1.1.1
+cycler==0.12.1
+Cython==3.0.11
+docker-pycreds==0.4.0
+docstring_parser==0.16
+etils==1.3.0
+eval_type_backport==0.2.2
+Farama-Notifications==0.0.4
+fasteners==0.19
+filelock==3.16.1
+fonttools==4.55.3
+fsspec==2024.12.0
+gitdb==4.0.12
+GitPython==3.1.44
+glfw==2.8.0
+google-auth==2.37.0
+google-auth-oauthlib==1.0.0
+grpcio==1.69.0
+gym==0.23.1
+gym-notices==0.0.8
+gymnasium==0.28.1
+idna==3.10
+imageio==2.35.1
+importlib_metadata==8.5.0
+importlib_resources==6.4.5
+jax-jumpy==1.0.0
+Jinja2==3.1.5
+kiwisolver==1.4.7
+Markdown==3.7
+markdown-it-py==3.0.0
+MarkupSafe==2.1.5
 matplotlib==3.7.5
-mujoco==3.2.3
-mujoco-py==2.1.2.14
-opencv-python==4.10.0.84
-pip==21.0
-PySocks==1.7.1
-tensorflow==2.13.1
-torchaudio==2.0.2+cu118
-torchrl==0.5.0
-torchvision==0.15.2+cu118
-wandb==0.18.3
+mdurl==0.1.2
+mpmath==1.3.0
+mujoco==2.3.3
+networkx==3.1
+numpy==1.24.4
+nvidia-cublas-cu12==12.1.3.1
+nvidia-cuda-cupti-cu12==12.1.105
+nvidia-cuda-nvrtc-cu12==12.1.105
+nvidia-cuda-runtime-cu12==12.1.105
+nvidia-cudnn-cu12==9.1.0.70
+nvidia-cufft-cu12==11.0.2.54
+nvidia-curand-cu12==10.3.2.106
+nvidia-cusolver-cu12==11.4.5.107
+nvidia-cusparse-cu12==12.1.0.106
+nvidia-nccl-cu12==2.20.5
+nvidia-nvjitlink-cu12==12.6.85
+nvidia-nvtx-cu12==12.1.105
+oauthlib==3.2.2
+packaging==24.2
+pandas==2.0.3
+pathtools==0.1.2
+pillow==10.4.0
+protobuf==4.25.6
+psutil==6.1.1
+pyasn1==0.6.1
+pyasn1_modules==0.4.1
+pycparser==2.22
+Pygments==2.19.1
+PyOpenGL==3.1.7
+pyparsing==3.1.4
+python-dateutil==2.9.0.post0
+pytz==2024.2
+PyYAML==6.0.2
+requests==2.32.3
+requests-oauthlib==2.0.0
+rich==13.9.4
+rsa==4.9
+sentry-sdk==2.20.0
+setproctitle==1.3.4
+shtab==1.7.1
+six==1.17.0
+smmap==5.0.2
+stable_baselines3==2.4.1
+sympy==1.13.3
+tensorboard==2.14.0
+tensorboard-data-server==0.7.2
+torch==2.4.1
+triton==3.0.0
+typeguard==4.4.0
+typing_extensions==4.12.2
+tyro==0.9.6
+tzdata==2024.2
+urllib3==2.2.3
+wandb==0.13.11
+Werkzeug==3.0.6
+zipp==3.20.2
