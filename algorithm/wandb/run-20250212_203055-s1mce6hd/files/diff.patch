diff --git a/.vscode/launch.json b/.vscode/launch.json
index 8490e0a..6ec5265 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -1,24 +1,15 @@
 {
     "version": "0.2.0",
     "configurations": [
+        
+
         {
-            "name": "Python Debugger: train.py with Arguments",
+            "name": "Python Debugger: Python files",
             "type": "debugpy",
             "request": "launch",
-            "program": "${workspaceFolder}/train.py",
+            "program": "${workspaceFolder}/algorithm/sac_rrd_iq.py",
             "console": "integratedTerminal",
-            "args": [
-                "--tag","Atari_256_32",
-                "--alg", "rrd_atari_pytorch",
-                "--basis_alg", "dqn",
-                "--code","pytorch",
-                "--rrd_bias_correction", "True",
-                "--env", "Assault",
-                "--rrd_batch_size","1024",
-                "--rrd_sample_size","32",
-
-
-            ]
+            
         }
     ]
 }
diff --git a/algorithm/rrd_mujoco_pytorch.py b/algorithm/rrd_mujoco_pytorch.py
index d0e9181..75321c3 100644
--- a/algorithm/rrd_mujoco_pytorch.py
+++ b/algorithm/rrd_mujoco_pytorch.py
@@ -221,7 +221,7 @@ def RRD_mujoco_pytorch(args):
             # Calculate Q-value for next state-action pair
             q_value_next = self._calculate_q_value(next_obs_ph, next_acts)
  
-            # Reward calculation: r = Q(s, a) - γ * Q(s', a')
+            # Reward calculation: r = Q(s, a) - γ * V(s')
             reward = q_value - self.args.gamma * q_value_next
  
             # Reshape if input was flattened
